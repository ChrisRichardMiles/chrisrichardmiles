{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_no_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp m5.daily_models\n",
    "#export\n",
    "import os\n",
    "import logging\n",
    "import collections\n",
    "import gc\n",
    "import json\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "from lightgbm.callback import record_evaluation\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import neptune.new as neptune\n",
    "from neptune.new.integrations.lightgbm import NeptuneCallback, create_booster_summary\n",
    "from fastcore.script import call_parse, Param\n",
    "\n",
    "from chrisrichardmiles.core import load_features\n",
    "from chrisrichardmiles.m5.fe import make_grid_df\n",
    "from chrisrichardmiles.m5.metric import WRMSSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def main(path_cfg): \n",
    "    if type(path_cfg) == str:\n",
    "        with open(path_cfg, 'r') as f: \n",
    "            cfg = json.load(f)\n",
    "    else: \n",
    "        cfg = path_cfg\n",
    "    with open(cfg['features_json'], 'r') as f: \n",
    "        dict_features = json.load(f)\n",
    "\n",
    "    # if type(cfg['start_test']) == list: \n",
    "    #     for start_test in cfg['start_test']: \n",
    "    #         tmp_cfg = cfg.copy()\n",
    "    #         tmp_cfg['start_test'] = start_test\n",
    "    #         lgb_daily(tmp_cfg, path_features, cfg['path_data'])\n",
    "\n",
    "    MODEL_NAME = 'lgb_day_by_day'\n",
    "    target = 'sales'\n",
    "    p_horizon = 28\n",
    "    num_series = 30490\n",
    "    start_train = cfg['start_train']\n",
    "\n",
    "    df_stv = pd.read_csv(os.path.join(cfg['path_data_raw'], 'sales_train_evaluation.csv'))\n",
    "    grid_df, _ = make_grid_df(df_stv)\n",
    "\n",
    "    ###### Only use items with at least 68 days of sales ######\n",
    "    first_sale = grid_df[grid_df.sales.notnull()].drop_duplicates('id')\n",
    "    keep_id = first_sale[(cfg['start_test'] - first_sale.d) >= 68].id.tolist()\n",
    "    df_stv_trunc = df_stv[df_stv.id.isin(keep_id)]\n",
    "    grid_df = grid_df[grid_df.id.isin(keep_id)]\n",
    "    del first_sale\n",
    "    gc.collect()\n",
    "\n",
    "    #################### full valid and test sets ###################\n",
    "    valid_days = [cfg['start_test'] + d - p_horizon for d in range(p_horizon)]\n",
    "    valid_actuals = df_stv_trunc[[f'd_{d}' for d in valid_days]].values\n",
    "    e = WRMSSE(cfg['path_data_raw'], cfg['start_test'], df_stv_trunc=df_stv_trunc)\n",
    "    if cfg['fobj_weight_col'] == 'total_scaled_weight':\n",
    "        e.add_total_scaled_weight()\n",
    "\n",
    "    if cfg['start_test'] != 1942: test_actuals = e.actuals.copy() \n",
    "    prediction_df = df_stv_trunc[['id']]\n",
    "\n",
    "    del df_stv, df_stv_trunc, e.df_stv\n",
    "    gc.collect()\n",
    "\n",
    "    # To avoid problems with the validation and test sets, \n",
    "    # we fill nan in valid and test period in case we have \n",
    "    # any nans from fixing out-of-stock data.\n",
    "    test_days = [cfg['start_test'] + i for i in range(28)]\n",
    "    mask = grid_df.d.isin(test_days + valid_days) & grid_df[target].isna()\n",
    "    grid_df.loc[mask, target] = 0\n",
    "\n",
    "    # Remove data where sales is nan\n",
    "    grid_df = grid_df[grid_df[target].notnull()]\n",
    "    gc.collect()\n",
    "\n",
    "    # Keeping all data to subsample from fro day by day models\n",
    "    full_grid_df = grid_df.copy() \n",
    "\n",
    "    # For neptune: \n",
    "    dict_eval_logs = []\n",
    "    booster_summaries = []\n",
    "\n",
    "    ############### Day by day training and predicting #############\n",
    "    if cfg['days_to_predict'] == \"all\": cfg['days_to_predict'] = range(28)\n",
    "\n",
    "    for day_of_horizon in cfg['days_to_predict']:\n",
    "    # day_of_horizon = 0\n",
    "        # Starting with full data and filtering for same day of week\n",
    "        grid_df = full_grid_df.copy() \n",
    "        test_day = cfg['start_test'] + day_of_horizon\n",
    "        valid_day = test_day - 28\n",
    "        same_day_of_week = [d for d in range(cfg['start_train'], test_day + 1) if d%7 == (test_day)%7]\n",
    "        grid_df = grid_df[grid_df.d.isin(same_day_of_week)]\n",
    "        gc.collect()\n",
    "\n",
    "        ######################## Loading Features #######################\n",
    "        files = [f.replace('csv', 'pkl') for f in dict_features.keys()]\n",
    "        feature_cols = list(chain(*dict_features.values()))\n",
    "        if type(cfg['path_features']) == 'str': \n",
    "            cfg['path_features'] = [cfg['path_features']]\n",
    "        for folder in cfg['path_features']: \n",
    "            path = os.path.join('../input', folder)\n",
    "            for file in os.listdir(path): \n",
    "                if file in files:\n",
    "                    tmp = os.path.join(path, file)\n",
    "                    df = pd.read_pickle(tmp)\n",
    "                    keep_cols = [c for c in df.columns if c in feature_cols]\n",
    "                    df = df[keep_cols]\n",
    "                    if file.startswith('shift'): \n",
    "                        df.index += day_of_horizon * 30490\n",
    "                    df = df.reindex(grid_df.index)\n",
    "                    grid_df = pd.concat([grid_df, df], axis=1)\n",
    "        del df\n",
    "        gc.collect()\n",
    "\n",
    "        remove_features = ['id', 'd', target]\n",
    "        feature_cols = [col for col in list(grid_df) if col not in remove_features]\n",
    "\n",
    "        ################## test, train and valid set ####################\n",
    "        valid_mask = (grid_df.d == valid_day) \n",
    "        train_mask = (grid_df.d >= start_train) & (grid_df.d < valid_day) & (grid_df[target].notnull())\n",
    "        test_mask = (grid_df.d == test_day)\n",
    "\n",
    "        train_x, train_y = grid_df[train_mask][feature_cols], grid_df[train_mask][target]\n",
    "        valid_x, valid_y = grid_df[valid_mask][feature_cols], grid_df[valid_mask][target]\n",
    "        test_x, test_y = grid_df[test_mask][feature_cols], grid_df[test_mask][target]\n",
    "\n",
    "        ################## Fit custom objective and metric ##################\n",
    "        w_12_train = e.w_12.reindex(grid_df[train_mask].id)\n",
    "        w_12_eval = e.w_12.reindex(grid_df[valid_mask].id)\n",
    "        w_12_test = e.w_12.reindex(grid_df[test_mask].id)\n",
    "\n",
    "        if cfg['fobj']: \n",
    "            get_fobj = getattr(e, f'get_weighted_{cfg[\"fobj\"]}_fobj')\n",
    "            fobj = get_fobj(w_12_train, cfg['fobj_weight_col'], cfg['weight_hess'])\n",
    "        else: \n",
    "            fobj = None\n",
    "\n",
    "        if cfg['feval']:\n",
    "            if cfg['feval'] == 'feval': \n",
    "                feval = e.feval\n",
    "            else: \n",
    "                get_feval = getattr(e, f'get_weighted_{cfg[\"feval\"]}_feval')\n",
    "                feval = get_feval(w_12_eval, cfg['feval_weight_col'])  \n",
    "        else: \n",
    "            feval = None\n",
    "\n",
    "        # Set evaluator actuals to valid day for early stopping\n",
    "        e.actuals = valid_actuals[:, day_of_horizon].reshape((-1,1))\n",
    "\n",
    "        ############# lightgbm datasets for training #############\n",
    "        if cfg['weight_col']: \n",
    "            weight_train = w_12_train[cfg['weight_col']].values\n",
    "            weight_eval = w_12_eval[cfg['weight_col']].values\n",
    "            weight_test = w_12_test[cfg['weight_col']].values\n",
    "        else: \n",
    "            weight_train, weight_eval, weight_test = None, None, None\n",
    "        train_data = lgb.Dataset(train_x, label=train_y, weight=weight_train)\n",
    "        valid_data = lgb.Dataset(valid_x, label=valid_y, weight=weight_eval, reference = train_data)\n",
    "#         test_data = lgb.Dataset(test_x, label=test_y, weight=weight_test)\n",
    "\n",
    "        del grid_df, train_x, train_y, valid_x, valid_y, \\\n",
    "            w_12_train, w_12_eval, w_12_test, \\\n",
    "            weight_train, weight_eval, weight_test\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "        ####################### Training ##########################\n",
    "        # Callbacks\n",
    "        dict_eval_log = {}\n",
    "        # neptune_callback = NeptuneCallback(run=run)\n",
    "\n",
    "        estimator = lgb.train(\n",
    "            cfg['lgb_params'],\n",
    "            train_set=train_data,\n",
    "            valid_sets=[valid_data],\n",
    "            valid_names=['valid'],\n",
    "            fobj = fobj,\n",
    "            feval = feval,\n",
    "            callbacks=[record_evaluation(dict_eval_log)],\n",
    "            verbose_eval=500\n",
    "        )\n",
    "\n",
    "        preds = estimator.predict(test_x)\n",
    "        prediction_df[f'F{day_of_horizon + 1}'] = preds\n",
    "        gc.collect()\n",
    "\n",
    "        booster_summary = create_booster_summary(\n",
    "            booster=estimator, \n",
    "            max_num_features=50, \n",
    "            log_trees_as_dataframe=False\n",
    "        )\n",
    "        booster_summaries.append(booster_summary)\n",
    "        dict_eval_logs.append(dict_eval_log)\n",
    "        del booster_summary, dict_eval_log\n",
    "        gc.collect()\n",
    "\n",
    "    # Saving predictions in submission ready format\n",
    "    tmp = prediction_df.copy()\n",
    "    prediction_df.id = prediction_df.id.str.replace('evaluation', 'validation')\n",
    "    #     path = os.path.join(cfg['path_models'], MODEL_NAME + '_' + path_cfg, 'preds.csv')\n",
    "    #     os.makedirs(path, exist_ok=True)\n",
    "    path = MODEL_NAME + '_' + path_cfg.split('.')[0] + '_' + 'submission.csv'\n",
    "    prediction_df = pd.concat([tmp, prediction_df])\n",
    "    prediction_df.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "@call_parse\n",
    "def lgb_daily(path_cfg: Param('path to the configuration json', str)='cfg.json'):\n",
    "    if type(path_cfg) == str:\n",
    "        with open(path_cfg, 'r') as f: \n",
    "            cfg = json.load(f)\n",
    "    else: \n",
    "        cfg = path_cfg\n",
    "    with open(cfg['features_json'], 'r') as f: \n",
    "        dict_features = json.load(f)\n",
    "\n",
    "    # if type(cfg['start_test']) == list: \n",
    "    #     for start_test in cfg['start_test']: \n",
    "    #         tmp_cfg = cfg.copy()\n",
    "    #         tmp_cfg['start_test'] = start_test\n",
    "    #         lgb_daily(tmp_cfg, path_features, cfg['path_data'])\n",
    "\n",
    "    MODEL_NAME = 'lgb_day_by_day'\n",
    "    target = 'sales'\n",
    "    p_horizon = 28\n",
    "    num_series = 30490\n",
    "    start_train = cfg['start_train']\n",
    "\n",
    "    df_stv = pd.read_csv(os.path.join(cfg['path_data_raw'], 'sales_train_evaluation.csv'))\n",
    "    grid_df, _ = make_grid_df(df_stv)\n",
    "\n",
    "    ###### Only use items with at least 68 days of sales ######\n",
    "    first_sale = grid_df[grid_df.sales.notnull()].drop_duplicates('id')\n",
    "    keep_id = first_sale[(cfg['start_test'] - first_sale.d) >= 68].id.tolist()\n",
    "    df_stv_trunc = df_stv[df_stv.id.isin(keep_id)]\n",
    "    grid_df = grid_df[grid_df.id.isin(keep_id)]\n",
    "\n",
    "    #################### full valid and test sets ###################\n",
    "    valid_days = [cfg['start_test'] + d - p_horizon for d in range(p_horizon)]\n",
    "    valid_actuals = df_stv_trunc[[f'd_{d}' for d in valid_days]].values\n",
    "    e = WRMSSE(cfg['path_data_raw'], cfg['start_test'], df_stv_trunc=df_stv_trunc)\n",
    "    if cfg['fobj_weight_col'] == 'total_scaled_weight':\n",
    "        e.add_total_scaled_weight()\n",
    "\n",
    "    if cfg['start_test'] != 1942: test_actuals = e.actuals.copy() \n",
    "    prediction_df = df_stv_trunc[['id']]\n",
    "\n",
    "    # Keeping all data to subsample from fro day by day models\n",
    "    full_grid_df = grid_df.copy() \n",
    "\n",
    "    # For neptune: \n",
    "    dict_eval_logs = []\n",
    "    booster_summaries = []\n",
    "\n",
    "    ############### Day by day training and predicting #############\n",
    "    if cfg['days_to_predict'] == \"all\": cfg['days_to_predict'] = range(28)\n",
    "    for day_of_horizon in cfg['days_to_predict']:\n",
    "    #     day_of_horizon = 0\n",
    "\n",
    "        # Starting with full data and filtering for same day of week\n",
    "        grid_df = full_grid_df.copy() \n",
    "        test_day = cfg['start_test'] + day_of_horizon\n",
    "        valid_day = test_day - 28\n",
    "        same_day_of_week = [d for d in range(cfg['start_train'], test_day + 1) if d%7 == (test_day)%7]\n",
    "        grid_df = grid_df[grid_df.d.isin(same_day_of_week)]\n",
    "\n",
    "        ######################## Loading Features #######################\n",
    "        index = grid_df.index.tolist()\n",
    "        grid_df = pd.concat([\n",
    "            grid_df, \n",
    "            load_features(cfg['path_features'], dict_features, reindex_with=index, shift_index=num_series * day_of_horizon)\n",
    "        ], axis=1)\n",
    "\n",
    "        remove_features = ['id', 'd', target]\n",
    "        feature_cols = [col for col in list(grid_df) if col not in remove_features]\n",
    "\n",
    "        ################## test, train and valid set ####################\n",
    "        valid_mask = (grid_df.d == valid_day) \n",
    "        train_mask = (grid_df.d >= start_train) & (grid_df.d < valid_day) & (grid_df[target].notnull())\n",
    "        test_mask = (grid_df.d == test_day)\n",
    "\n",
    "        train_x, train_y = grid_df[train_mask][feature_cols], grid_df[train_mask][target]\n",
    "        valid_x, valid_y = grid_df[valid_mask][feature_cols], grid_df[valid_mask][target]\n",
    "        test_x, test_y = grid_df[test_mask][feature_cols], grid_df[test_mask][target]\n",
    "\n",
    "        ################## Fit custom objective and metric ##################\n",
    "        w_12_train = e.w_12.reindex(grid_df[train_mask].id)\n",
    "        w_12_eval = e.w_12.reindex(grid_df[valid_mask].id)\n",
    "        w_12_test = e.w_12.reindex(grid_df[test_mask].id)\n",
    "\n",
    "        if cfg['fobj']: \n",
    "            get_fobj = getattr(e, f'get_weighted_{cfg[\"fobj\"]}_fobj')\n",
    "            fobj = get_fobj(w_12_train, cfg['fobj_weight_col'], cfg['weight_hess'])\n",
    "        else: \n",
    "            fobj = None\n",
    "\n",
    "        if cfg['feval']:\n",
    "            if cfg['feval'] == 'feval': \n",
    "                feval = e.feval\n",
    "            else: \n",
    "                get_feval = getattr(e, f'get_weighted_{cfg[\"feval\"]}_feval')\n",
    "                feval = get_feval(w_12_eval, cfg['feval_weight_col'])  \n",
    "        else: \n",
    "            feval = None\n",
    "\n",
    "        # Set evaluator actuals to valid day for early stopping\n",
    "        e.actuals = valid_actuals[:, day_of_horizon].reshape((-1,1))\n",
    "\n",
    "        ############# lightgbm datasets for training #############\n",
    "        if cfg['weight_col']: \n",
    "            weight_train = w_12_train[cfg['weight_col']].values\n",
    "            weight_eval = w_12_eval[cfg['weight_col']].values\n",
    "            weight_test = w_12_test[cfg['weight_col']].values\n",
    "        else: \n",
    "            weight_train, weight_eval, weight_test = None, None, None\n",
    "        train_data = lgb.Dataset(train_x, label=train_y, weight=weight_train)\n",
    "        valid_data = lgb.Dataset(valid_x, label=valid_y, weight=weight_eval)\n",
    "        test_data = lgb.Dataset(test_x, label=test_y, weight=weight_test)\n",
    "\n",
    "\n",
    "        ####################### Training ##########################\n",
    "        # Callbacks\n",
    "        dict_eval_log = {}\n",
    "        # neptune_callback = NeptuneCallback(run=run)\n",
    "\n",
    "        estimator = lgb.train(\n",
    "            cfg['lgb_params'],\n",
    "            train_set=train_data,\n",
    "            valid_sets=[valid_data, test_data],\n",
    "            valid_names=['valid', 'test'],\n",
    "            fobj = fobj,\n",
    "            feval = feval,\n",
    "            callbacks=[record_evaluation(dict_eval_log)],\n",
    "            verbose_eval=500\n",
    "        )\n",
    "        booster_summary = create_booster_summary(booster=estimator, max_num_features=50)\n",
    "        booster_summary = create_booster_summary(booster=estimator, max_num_features=50)\n",
    "        booster_summaries.append(booster_summary)\n",
    "        dict_eval_logs.append(dict_eval_log)\n",
    "\n",
    "        preds = estimator.predict(test_x)\n",
    "        prediction_df[f'F{day_of_horizon + 1}'] = preds\n",
    "        gc.collect()    \n",
    "\n",
    "    # Saving predictions in submission ready format\n",
    "    tmp = prediction_df.copy()\n",
    "    prediction_df.id = prediction_df.id.str.replace('evaluation', 'validation')\n",
    "#     path = os.path.join(cfg['path_models'], MODEL_NAME + '_' + path_cfg, 'preds.csv')\n",
    "#     os.makedirs(path, exist_ok=True)\n",
    "    path = MODEL_NAME + '_' + path_cfg.split('.')[0] + '_' + 'submission.csv'\n",
    "    prediction_df.to_csv(path, index=False)\n",
    "\n",
    "    ############## Neptune experiment tracking ###############\n",
    "    if cfg['use_neptune']: \n",
    "        run = neptune.init(\n",
    "            project=cfg['neptune_project'], \n",
    "            api_token=cfg['neptune_api_token'], \n",
    "            custom_run_id='test', \n",
    "            name='Untitled',\n",
    "            description='',\n",
    "            tags=['testing', '2test'], \n",
    "            source_files=[cfg['features_json'], path_cfg],\n",
    "        )\n",
    "#     else: \n",
    "#         run = {}\n",
    "\n",
    "        if cfg['start_test'] != 1942: \n",
    "            e.actuals = test_actuals\n",
    "            run['WRMSSE'] = e.score(prediction_df.iloc[:, 1:].values)\n",
    "\n",
    "        # General params\n",
    "        for k, v in cfg.items(): \n",
    "            if k not in ['lgb_params', 'neptune_project', 'neptune_api_token']: \n",
    "                run[k] = v\n",
    "\n",
    "        # Logs for each days model\n",
    "        for i, day in enumerate(cfg[\"days_to_predict\"]): \n",
    "            run[f'lgbm_summary/day_{day}'] = booster_summaries[i]\n",
    "\n",
    "            for valid_set, odict in dict_eval_log.items(): \n",
    "                for metric, log in odict.items(): \n",
    "                    for val in log: \n",
    "                        run[f'eval_logs/{day}/{valid_set}/{metric}'].log(val)\n",
    "        run.stop()\n",
    "#         if cfg['use_neptune']: run.stop()\n",
    "#     else: \n",
    "# #         path = os.path.join(cfg['path_models'], MODEL_NAME + '_' + path_cfg, 'run.json')\n",
    "# #         os.makedirs(path, exist_ok=True)\n",
    "#         path = MODEL_NAME + '_' + path_cfg.split('.')[0] + '_' + 'run.json'\n",
    "#         with open(path, 'w') as f: \n",
    "#             json.dump(run, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data was loaded\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's l2: 1.20148\tvalid's mse_feval_scale: 1.30647\ttest's l2: 1.54952\ttest's mse_feval_scale: 1.54952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chrisrichardmiles/miniconda3/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/home/chrisrichardmiles/miniconda3/lib/python3.9/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Booster's feature_importance is empty.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4683/970003490.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlgb_daily\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'final_cfg.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_4683/486184396.py\u001b[0m in \u001b[0;36mlgb_daily\u001b[0;34m(path_cfg)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         )\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mbooster_summary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_booster_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbooster\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_num_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0mbooster_summary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_booster_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbooster\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_num_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mbooster_summaries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbooster_summary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/neptune_lightgbm/impl/__init__.py\u001b[0m in \u001b[0;36mcreate_booster_summary\u001b[0;34m(booster, log_importances, max_num_features, list_trees, log_trees_as_dataframe, log_pickled_booster, log_trees, tree_figsize, log_confusion_matrix, y_true, y_pred)\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0mvisuals_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"visualizations/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlog_importances\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m         split_plot = lgb.plot_importance(\n\u001b[0m\u001b[1;32m    335\u001b[0m             \u001b[0mbooster\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mimportance_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"split\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/lightgbm/plotting.py\u001b[0m in \u001b[0;36mplot_importance\u001b[0;34m(booster, ax, height, xlim, ylim, title, xlabel, ylabel, importance_type, max_num_features, ignore_zero, figsize, dpi, grid, precision, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimportance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Booster's feature_importance is empty.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0mtuples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimportance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Booster's feature_importance is empty."
     ]
    }
   ],
   "source": [
    "lgb_daily('final_cfg.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('lgb_day_by_day_final_cfg_submission.csv')\n",
    "# df.F1.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neptune code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ.get??\n",
    "# print(os.environ.get('NEPTUNE_API_TOKEN'))\n",
    "# neptune.init()\n",
    "# lgb.train??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
