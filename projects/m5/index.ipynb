{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to crm_m5\n",
    "\n",
    "> A package built for the M5-Accuracy competition on Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install chrisrichardmiles`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use with command line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create data folders, download data, and unzip files\n",
    "* If you have your kaggle api info in root/.kaggle/kaggle.json then run: \n",
    "```\n",
    "crm_download_kaggle_data --comp_name m5-forecasting-accuracy\n",
    "```\n",
    "* Otherwise, you must run: \n",
    "```\n",
    "crm_mkdirs_data\n",
    "cd data/raw\n",
    "```\n",
    " - Now manully download the data zipfile from [kaggle](https://www.kaggle.com/c/m5-forecasting-accuracy/data) and upload it into the data/raw folder.\n",
    "```\n",
    "unzip * \n",
    "cd ../..\n",
    "```\n",
    "\n",
    "### 2. Fill the out of stock\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some lessons I learned "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAM issues \n",
    " * Computing rolling window statistics can be very expensive, but can be ok if we do processing in smaller sections like we do with `n_splits` in the `rolling_window` statistics calculations.\n",
    " * Datatype matter: float32 and float16 datatypes can save a lot of RAM, but be careful for float16, which might cause accuracy problems or completely break a process, like when trying to use `StandardScaler` with float16 datatypes resulted in all zeros. \n",
    " * Be careful with Pandas DataFrames. I had a lot of problems where I was unmindfully creating large copies of data, such as a saving function, looking something like `df[cols].to_csv(....)` which was making an entirely new dataframe in memory before saving. This was remedied by using the `usecols` param in `df.to_csv`\n",
    " * Be careful with DataFrame names. It seems that it is better to keep the same name of a DataFrame when doing things like concating an existing df with new data. Its like pandas will use memory more intelligently. So this: `df2 = pd.concat([df1, pd.read_csv('data.csv')]; del df2` should be replaced with `df1 = pd.concat([df1, pd.read_csv('data.csv')]`. It seems like it should work the same, but my experience with RAM crashes seems to indicate the second method is much better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organization is hard for me, but most important\n",
    "The first iteration of this project was code that was spread accross hundreds of kaggle notebooks. I ran experiments by running a notebook and just looking at the result. It always felt ok while I was doing it, and it worked since the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
