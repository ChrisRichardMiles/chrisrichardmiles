{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Make tidy features from wide time series data.\n",
    "output-file: feature_engineering.html\n",
    "title: Feature engineering\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp m5.fe\n",
    "#| export\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import time\n",
    "import logging\n",
    "from typing import Union\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "from fastcore.script import call_parse, Param\n",
    "\n",
    "from chrisrichardmiles.core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "plt.rcParams['figure.figsize'] = (14,6)\n",
    "plt.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATA = 'data'\n",
    "PATH_DATA_RAW = 'data/raw'\n",
    "PATH_DATA_FEATURES = 'data/features'\n",
    "os.listdir(PATH_DATA_RAW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## Load data ####################\n",
    "chunks = pd.read_csv(os.path.join(PATH_DATA_RAW, 'sales_train_evaluation.csv'), chunksize=1000)\n",
    "df_stv = pd.concat(list(chunks)) # Safe for low RAM situation\n",
    "df_cal = pd.read_csv(os.path.join(PATH_DATA_RAW, 'calendar.csv'))\n",
    "df_prices = pd.read_csv(os.path.join(PATH_DATA_RAW, 'sell_prices.csv'))\n",
    "df_ss = pd.read_csv(os.path.join(PATH_DATA_RAW, 'sample_submission.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **What you can get out of this notebook**\n",
    "\n",
    "1. Know how to make lag features from the horizontal \"rectangle\" data representation, which is how the data starts.\n",
    "2. Knoweldge of how to utilize numpy to do quick rolling window aggregations.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a grid to align all features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section develops code for `make_grid_df` which will yield: \n",
    " * A dataframe to align all features\n",
    " * A numpy array where each row is a time series. This data representation can be good for for fast feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add prediction horizon\n",
    "We will start by adding the prediction horizon to our original data so that feature our features will be generated all training data and our test data at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_day = int(df_stv.columns[-1][2:])\n",
    "pred_horizon = 28\n",
    "for i in range(last_day + 1, last_day + 1 + pred_horizon): \n",
    "    df_stv[f'd_{i}'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a tidy grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want our data in a tidy format, where we have a row for every \n",
    "product/sales_day combination. To do this, we start be reshaping \n",
    "our data to long format. I will call this our `grid_df`, on which we\n",
    "will build our features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using pandas \n",
    "We can use pandas dataframe `.melt` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "s = time.time()\n",
    "start_time = time.time()\n",
    "DROP_COLS = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n",
    "grid_df = df_stv.drop(DROP_COLS, axis=1).melt(id_vars='id', var_name='d', value_name='sales')\n",
    "# print(f\"Total time for melt: {(time.time() - start_time)/60} min\")\n",
    "print(f\"Total time for melt: \", time_taken(start_time))\n",
    "\n",
    "\n",
    "# Saving space\n",
    "start_time = time.time()\n",
    "grid_df['d'] = grid_df.d.str[2:].astype(np.int16)\n",
    "print(f\"Total time for day col change: \", time_taken(start_time))\n",
    "\n",
    "start_time = time.time()\n",
    "grid_df['id'] = grid_df.id.astype('category')\n",
    "print(f\"Total time for category: \", time_taken(start_time))\n",
    "\n",
    "print(time_taken(s))\n",
    "display(grid_df)\n",
    "\n",
    "del s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Faster grid ceation using numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "d_cols = [col for col in df_stv.columns if col.startswith('d_')]\n",
    "g = pd.DataFrame({'id': pd.Series(np.tile(df_stv.id, len(d_cols))).astype('category'), \n",
    "                  'd': np.concatenate([[int(s[2:])] * df_stv.shape[0] for s in d_cols]).astype(np.int16), \n",
    "                  'sales': df_stv[d_cols].values.T.reshape(-1,)})\n",
    "\n",
    "print(f'Both grids are the same: {grid_df.equals(g)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolate numpy array in \"rectangle\" representation\n",
    "\n",
    "I will take the sales values as they are to \n",
    "form my base \"rectangle\" of sales. \n",
    "I think I can take this recatangle and \n",
    "quickly reshape it so that it lines up \n",
    "with grid_df. If I am correct we can use this \n",
    "to create features quickly. \n",
    "\n",
    "**Test**:\n",
    "Reshape the basic \n",
    "rectangle so that it matches sales of `grid_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = df_stv[d_cols].values\n",
    "test_sales = rec.T.reshape(-1)\n",
    "print('test_sales matches sales?? ', (np.nan_to_num(test_sales) == grid_df['sales'].fillna(0)).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The competition guide states that leading zeros sales should not be considered, therefore we need to convert these leading zeros to NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def nan_leading_zeros(rec):\n",
    "    \"\"\"Leading zeros indicate an item was not for sale. We \n",
    "    will mark as np.nan to ensure they are not used for training.\"\"\"\n",
    "    \n",
    "    rec = rec.astype(np.float64)\n",
    "    zero_mask = rec.cumsum(axis=1) == 0\n",
    "    rec[zero_mask] = np.nan\n",
    "    return rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rec[: 10, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_leading_zeros(rec[: 10, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def make_grid_df(df: pd.DataFrame, \n",
    "                 pred_horizon=28) -> (pd.DataFrame, np.array): \n",
    "    \"\"\"Specific to the the M5 competition data. \n",
    "    Returns a grid_df to allign all features and the sales  \n",
    "    data in a \"rectangle\" data representation, a 2D numpy array \n",
    "    where ever row is an items time series.\n",
    "    \"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    logging.info(\"#\" * 72)\n",
    "    logging.info(\"Making grid_df\")\n",
    "    \n",
    "    # If passed a path to raw data instead of a dataframe, create df\n",
    "    if type(df) == str:\n",
    "        df = pd.read_csv(df)\n",
    "    else: \n",
    "        df = df.copy()\n",
    "    \n",
    "    start_test = int(df.columns[-1][2:]) + 1\n",
    "    if pred_horizon: \n",
    "        for i in range(start_test, start_test + pred_horizon): \n",
    "            df[f'd_{i}'] = np.nan\n",
    "    \n",
    "    d_cols = [col for col in df.columns if col.startswith('d_')]\n",
    "    \n",
    "    # Turn leading zeros into np.nan\n",
    "    rec = nan_leading_zeros(df[d_cols].values)\n",
    "    sales = rec.T.reshape(-1,)\n",
    "    \n",
    "    grid_df = pd.DataFrame({\n",
    "        'id': pd.Series(np.tile(df.id, len(d_cols))).astype('category'), \n",
    "        'd': np.concatenate([[int(s[2:])] * df.shape[0] for s in d_cols]).astype(np.int16), \n",
    "        'sales': sales.astype(np.float16)\n",
    "    })\n",
    "    \n",
    "    logging.info(time_taken(start_time))\n",
    "    return grid_df, rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df, rec = make_grid_df(os.path.join(PATH_DATA_RAW, 'sales_train_evaluation.csv'), pred_horizon=28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base features\n",
    "> Functions to create basic calendar and price features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with a grid_df so that all our \n",
    "features will be aligned on the same index \n",
    "making it easy to add features for trianing. \n",
    "grid_df was created in the last few cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base categorical variables given by heierarchical levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will add the grouping levels \n",
    "of the data as features. This is easy \n",
    "because the features are already included \n",
    "in df_stv columns. We just need to make a copy of \n",
    "these columns for every day of training and \n",
    "prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def add_base(grid_df, df_stv, rec): \n",
    "    \"\"\" Adds the basic categorical features to grid_df. \"\"\"\n",
    "    logging.info('#' * 72)\n",
    "    logging.info('Adding basic categorical features to grid_df')\n",
    "    start_time = time.time()\n",
    "    for col in ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']: \n",
    "        grid_df[col] = pd.Series(np.tile(df_stv[col], rec.shape[1])).astype('category')\n",
    "    logging.info(time_taken(start_time))\n",
    "    return grid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_base(grid_df, df_stv, rec)\n",
    "grid_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Price features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def create_price_fe(df_prices): \n",
    "    \"\"\"Adds price features onto price_df. This is the step we take \n",
    "    before merging prices onto our grid_df.\"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    logging.info('#' * 72)\n",
    "    logging.info('Creating price features')\n",
    "    ################# Standard statistics #######################\n",
    "    group = df_prices.groupby(['item_id', 'store_id'])\n",
    "    df_prices['price_min'] =    group['sell_price'].transform('min').astype('float16')\n",
    "    df_prices['price_max'] =    group['sell_price'].transform('max').astype('float16')\n",
    "    df_prices['price_median'] = group['sell_price'].transform('median').astype('float16')\n",
    "    df_prices['price_mode'] =   group['sell_price'].transform(lambda x: x.value_counts().idxmax()).astype('float16')\n",
    "    df_prices['price_mean'] =   group['sell_price'].transform('mean').astype('float16')\n",
    "    df_prices['price_std'] =    group['sell_price'].transform('std').astype('float16')\n",
    "    df_prices['sell_price'] =   df_prices['sell_price'].astype('float16')\n",
    "    \n",
    "    ################ Normalization of sell_price ###############\n",
    "    df_prices['price_norm_max'] = (df_prices['sell_price'] / df_prices['price_max']).astype('float16')\n",
    "    df_prices['price_norm_mode'] = (df_prices['sell_price'] / df_prices['price_mode']).astype('float16')\n",
    "    df_prices['price_norm_mean'] = (df_prices['sell_price'] / df_prices['price_mean']).astype('float16')\n",
    "\n",
    "    ################# Momentum ########################\n",
    "    # Lets also try to incroporate some week to week price \n",
    "    # comparison, which we will call 'momentum'. \n",
    "    df_prices['price_momentum'] = (df_prices.sell_price / df_prices.groupby(['item_id', 'store_id'])['sell_price']\\\n",
    "        .transform(lambda x: x.shift(1))).astype('float16')\n",
    "\n",
    "    # We also want to compare the sell price to recent \n",
    "    # rolling averages such as the last 4 weeks and 24 weeks. \n",
    "    df_prices['price_roll_momentum_4'] = (df_prices.sell_price / df_prices.groupby(['item_id', 'store_id'])['sell_price']\\\n",
    "        .transform(lambda x: x.rolling(4).mean())).astype('float16')\n",
    "    df_prices['price_roll_momentum_24'] = (df_prices.sell_price / df_prices.groupby(['item_id', 'store_id'])['sell_price']\\\n",
    "        .transform(lambda x: x.rolling(24).mean())).astype('float16')\n",
    "\n",
    "    ################### Last digits #####################\n",
    "    # Its possible the last two digits of the price \n",
    "    # indicate if the product is on sale or clearance.  \n",
    "    df_prices['price_end_digits'] = ((df_prices.sell_price - np.floor(df_prices.sell_price)) * 100).astype('float16')\n",
    "\n",
    "    logging.info(time_taken(start_time))\n",
    "    return df_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prices = create_price_fe(df_prices)\n",
    "df_prices.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def add_price_fe(grid_df, df_prices, df_cal): \n",
    "    \"\"\" Adds on price features to grid_df. \"\"\"\n",
    "    \n",
    "    logging.info('#' * 72)\n",
    "    logging.info('Adding price features to grid')\n",
    "    start_time = time.time()\n",
    "    df_cal = df_cal.copy()\n",
    "\n",
    "    # df_prices needs an id column to merge with grid_df\n",
    "    df_prices['id'] = df_prices.item_id + '_' + df_prices.store_id + '_evaluation'    \n",
    "    \n",
    "    # Calendar d column must be int to match grid_df\n",
    "    if type(df_cal.d[1]) != np.int16:\n",
    "        df_cal['d'] = np.int16(df_cal.d.str[2:])\n",
    "        \n",
    "    # Grid_df will need a 'wm_yr_wk' column\n",
    "    grid_df = merge_by_concat(grid_df, df_cal[['wm_yr_wk', 'd']], ['d'])\n",
    "\n",
    "    # We want to make sure our features are in alignment \n",
    "    original_cols = list(grid_df)\n",
    "    grid_df = merge_by_concat(grid_df, df_prices, ['id', 'wm_yr_wk'])\n",
    "    keep_cols = [col for col in list(grid_df) if col not in original_cols and 'id' not in col]\n",
    "    grid_df = grid_df[['id', 'd', 'sales'] + keep_cols]\n",
    "\n",
    "    logging.info(time_taken(start_time))\n",
    "    return grid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df = add_price_fe(grid_df, df_prices, df_cal)\n",
    "grid_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calander Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df = grid_df[['id', 'd', 'sales']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def add_cal_fe(grid_df, df_cal): \n",
    "    \"\"\" Adds calendar features onto grid_df. \"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    logging.info('#' * 72)\n",
    "    logging.info('Creating calendar features grid')\n",
    "    \n",
    "    df_cal = df_cal.copy()\n",
    "\n",
    "    # Change calendar d column to int\n",
    "    if type(df_cal.d[1]) != np.int16:\n",
    "        df_cal['d'] = np.int16(df_cal.d.str[2:])\n",
    "    \n",
    "    ########### Merge part of calendar data ##############\n",
    "    grid_df = grid_df.merge(df_cal[[\n",
    "        'd', 'date','event_name_1', 'event_type_1', 'event_name_2', \n",
    "        'event_type_2','snap_CA', 'snap_TX', 'snap_WI'\n",
    "    ]], on='d', how='left')\n",
    "\n",
    "    ########## Categorify object columns ############\n",
    "    cat_cols = ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2','snap_CA', 'snap_TX', 'snap_WI']\n",
    "    for col in cat_cols: \n",
    "        grid_df[col] = grid_df[col].astype('category')\n",
    "        \n",
    "    ############# Create standard date features ##############\n",
    "    grid_df['date'] = pd.to_datetime(grid_df['date'])\n",
    "\n",
    "    # Make some features from date\n",
    "    grid_df['tm_d'] = grid_df['date'].dt.day.astype(np.int8)\n",
    "    grid_df['tm_w'] = grid_df['date'].dt.isocalendar().week.astype(np.int8)\n",
    "    grid_df['tm_m'] = grid_df['date'].dt.month.astype(np.int8)\n",
    "    grid_df['tm_y'] = grid_df['date'].dt.year\n",
    "    grid_df['tm_y'] = (grid_df['tm_y'] - grid_df['tm_y'].min()).astype(np.int8)\n",
    "    grid_df['tm_wm'] = grid_df['tm_d'].apply(lambda x: np.ceil(x/7)).astype(np.int8)\n",
    "    grid_df['tm_dw'] = grid_df['date'].dt.dayofweek.astype(np.int8)\n",
    "    grid_df['tm_w_end'] = (grid_df['tm_dw']>=5).astype(np.int8)\n",
    "\n",
    "    # Remove date\n",
    "    del grid_df['date']\n",
    "\n",
    "    # Change index back to RangeIndex to save space\n",
    "    grid_df.index = pd.RangeIndex(grid_df.index[-1] + 1)\n",
    "    \n",
    "    logging.info(time_taken(start_time))\n",
    "    return grid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df = add_cal_fe(grid_df, df_cal)\n",
    "grid_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snap features \n",
    "The columns with a name like 'snap_CA' indicates whether SNAP (also known as EBT) benefits are accepted in California for each day. What days can people use their SNAP benefits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_cal.iloc[-180:, :].copy()\n",
    "df['date'] = pd.to_datetime(df.date)\n",
    "fig, ax = plt.subplots(3, 1)\n",
    "df.set_index('date').snap_CA.plot(ax=ax[0], title='California')\n",
    "df.set_index('date').snap_TX.plot(ax=ax[1], title='Texas')\n",
    "df.set_index('date').snap_WI.plot(ax=ax[2], title='Wisconsin')\n",
    "\n",
    "ax[2].xaxis.set_major_locator(mdates.MonthLocator(bymonthday=1))\n",
    "ax[2].xaxis.set_major_formatter(mdates.DateFormatter(\"%b %Y\"))\n",
    "fig.suptitle('Days where people can use SNAP benefits')\n",
    "fig.autofmt_xdate()\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "df_cal['date'] = pd.to_datetime(df_cal['date'])\n",
    "df_cal.groupby(df_cal.date.dt.day)[['snap_CA', 'snap_TX', 'snap_WI']].sum().plot(\n",
    "    title='Are the snap days distributed accross these days for the entire dataset?',kind='bar', figsize=(14, 6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the snap days are consistent for every month in the dataset since each day has the same number of snap occurances for each date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Snap days of month ###########################\n",
    "ca = grid_df[grid_df['snap_CA'] == 1].tm_d.unique()\n",
    "tx = grid_df[grid_df['snap_TX'] == 1].tm_d.unique()\n",
    "wi = grid_df[grid_df['snap_WI'] == 1].tm_d.unique()\n",
    "print('For each state, what days of the month are snap days?')\n",
    "print('CA:', ca)\n",
    "print('TX:', tx)\n",
    "print('WI:', wi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple feature \n",
    "Just map each snap day to 1 through 10 so the \n",
    "model will know which of the 10 snap days it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def add_snap_transform_1(grid_df):\n",
    "    \"\"\"Adds a column that shows which of the 10 snap days it is. \n",
    "    The value is 0 if it is not a snap day.\n",
    "    \"\"\"\n",
    "    \n",
    "    ca = grid_df[grid_df['snap_CA'] == 1].tm_d.unique()\n",
    "    tx = grid_df[grid_df['snap_TX'] == 1].tm_d.unique()\n",
    "    wi = grid_df[grid_df['snap_WI'] == 1].tm_d.unique()\n",
    "\n",
    "    ################## Make mappers #################\n",
    "    ca_31 = list(ca) + [d for d in range(1,32) if d not in ca]\n",
    "    list_to_map_to = list(range(1,11)) + [0] * 21\n",
    "    ca_mapper = dict(zip(ca_31, list_to_map_to))\n",
    "\n",
    "    tx_31 = list(tx) + [d for d in range(1,32) if d not in tx]\n",
    "    list_to_map_to = list(range(1,11)) + [0] * 21\n",
    "    tx_mapper = dict(zip(tx_31, list_to_map_to))\n",
    "\n",
    "    wi_31 = list(wi) + [d for d in range(1,32) if d not in wi]\n",
    "    list_to_map_to = list(range(1,11)) + [0] * 21\n",
    "    wi_mapper = dict(zip(wi_31, list_to_map_to))\n",
    "\n",
    "    ################# Map ############################\n",
    "    grid_df.loc[grid_df.id.str.contains('CA'), 'snap_transform_1'] = grid_df.tm_d.map(ca_mapper)\n",
    "    grid_df.loc[grid_df.id.str.contains('TX'), 'snap_transform_1'] = grid_df.tm_d.map(tx_mapper)\n",
    "    grid_df.loc[grid_df.id.str.contains('WI'), 'snap_transform_1'] = grid_df.tm_d.map(wi_mapper)\n",
    "\n",
    "    #################### Save as int8 ################\n",
    "    grid_df['snap_transform_1'] = grid_df['snap_transform_1'].astype(np.int8)\n",
    "    return grid_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A more meaningful mapping?\n",
    "I would like to transform the snap information in a way that might give more information about non snap days. In particular, I want the \"gap\" days, non-snap days right in between two snap days, to be considered different from the long stretch of non-snap days towards the end of the month. I'd also like the non-snap days leading into the first snap day to be the same, no matter what state we are considering, so that algorithms can use this feature without needing state information to decode meaning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def add_snap_transform_2(grid_df): \n",
    "    \"\"\"This maps snap days and non snap days in way \n",
    "    that may be more meaningful than `snap_transform_1`.\n",
    "    \n",
    "    Any day above 40 will be a snap day. Lower days are non \n",
    "    snap, and lowest days are \"gap\" days in between\n",
    "    snap days. In this way I'm hoping the model can \n",
    "    can use this feature as non-categorical, and be \n",
    "    able to efficiently sort when higher demand days \n",
    "    may be. Also, 16-21 will always be the days \n",
    "    following the last snap day and 27-31 will be \n",
    "    the days leading up to the first snap day. My theory is\n",
    "    these numbers will encode more meaning with less \n",
    "    confusion caused by states having different snap days.\n",
    "    \"\"\"\n",
    "    ca = grid_df[grid_df['snap_CA'] == 1].tm_d.unique()\n",
    "    tx = grid_df[grid_df['snap_TX'] == 1].tm_d.unique()\n",
    "    wi = grid_df[grid_df['snap_WI'] == 1].tm_d.unique()\n",
    "    \n",
    "    ca_mapper = dict(zip(ca, ca + 40))\n",
    "    tx_mapper = dict(zip(tx, ca + 40))\n",
    "    wi_mapper = dict(zip(wi, ca + 40))\n",
    "\n",
    "    ns_ca = dict(zip([day for day in range(1,32) if day not in ca], \n",
    "                     [16,17,18,19,20,21] + [22,22,23,23,24,24,25,25,26,26] + [27, 28, 29, 30, 31]))\n",
    "    ns_tx = dict(zip([day for day in range(1,32) if day not in tx], \n",
    "                     list(range(1,6)) + list(range(16,32))))\n",
    "    ns_wi = dict(zip([day for day in range(1,32) if day not in wi], \n",
    "                     [31] + list(range(1,5)) + list(range(16,22)) + [22] + list(range(22,31))))\n",
    "\n",
    "    ca_mapper.update(ns_ca)\n",
    "    tx_mapper.update(ns_tx)\n",
    "    wi_mapper.update(ns_wi)\n",
    "\n",
    "    ################# Map ############################\n",
    "    grid_df.loc[grid_df.id.str.contains('CA'), 'snap_transform_2'] = grid_df.tm_d.map(ca_mapper)\n",
    "    grid_df.loc[grid_df.id.str.contains(\n",
    "        'TX'), 'snap_transform_2'] = grid_df.tm_d.map(tx_mapper)\n",
    "    grid_df.loc[grid_df.id.str.contains('WI'), 'snap_transform_2'] = grid_df.tm_d.map(wi_mapper)\n",
    "\n",
    "    #################### Save as int8 ################\n",
    "    grid_df['snap_transform_2'] = grid_df['snap_transform_2'].astype(np.int8)\n",
    "    return grid_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special event features \n",
    "How many days have events?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Type 1: {grid_df.event_type_1.count()/grid_df.shape[0] * 100:.2f} percent')\n",
    "print(f'Type 2: {grid_df.event_type_2.count()/grid_df.shape[0] * 100:.2f} percent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What special events do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Unique types in event_type_1:')\n",
    "display(grid_df.event_type_1.unique().tolist())\n",
    "print('Unique types in event_type_2:')\n",
    "display(grid_df.event_type_2.unique().tolist())\n",
    "print('Unique names in event_name_1:')\n",
    "display(grid_df.event_name_1.unique().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we ever have event_type_2 if there is not an even_type_1? \n",
    "\n",
    "How often do we have 2 events on the same day?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df[grid_df.event_name_2.notnull()].drop_duplicates('d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (grid_df['event_type_1'] == 'Religious') & (grid_df['event_type_2'] == 'Cultural')\n",
    "grid_df[mask].drop_duplicates('d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are only a few days with 2 events, I will only consider event_type_1 for my new event features. In the cases where I think the event_type_2 will be more relevant, I will move it to event_type_1. I think cultural event types may be more important than religious events. This will basically amount to making Easter cultural and Cinco De Mayo is also accounted for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def add_event_features(grid_df, n_items=30490, n_days_in_data=1969):\n",
    "    \"\"\"Adds some features related to special events like holidays to \n",
    "    grid_df. The columns added are: \n",
    "    next_event_type_1\n",
    "    last_event_type_1\n",
    "    days_since_event\n",
    "    days_until_event\n",
    "    \"\"\"\n",
    "    \n",
    "    # Replace event 1 with \"better\" event 2 on double event days.\n",
    "    mask = (grid_df['event_type_1'] == 'Religious') & (grid_df['event_type_2'] == 'Cultural')\n",
    "    grid_df.loc[mask, 'event_type_1'] = 'Cultural'\n",
    "\n",
    "    #################### Create next/last event features #######################\n",
    "    # I don't want to include the actual event, name because I think my model \n",
    "    # will learn what time of year it is, which I don't necessarily want it to \n",
    "    # do, because I think it will hurt its generalization.\n",
    "    grid_df['next_event_type_1'] = grid_df.event_type_1.fillna(method='bfill')\n",
    "    grid_df['last_event_type_1'] = grid_df.event_type_1.fillna(method='ffill')\n",
    "\n",
    "    ##################### Days since event feature ###########################\n",
    "    old_col = grid_df.event_type_1.cat.codes\n",
    "    counter = 1\n",
    "    new_col = []\n",
    "    for i in range(n_days_in_data): \n",
    "        if old_col[i*n_items] == -1: \n",
    "            new_col += [counter] * n_items\n",
    "            counter += 1\n",
    "        else: \n",
    "            new_col += [0] * n_items\n",
    "            counter = 1\n",
    "    grid_df['days_since_event'] = np.array(new_col).astype(np.int16)\n",
    "\n",
    "    ############### Days until event feature ######################## \n",
    "    old_col = grid_df.event_type_1.cat.codes[::-1].tolist()\n",
    "    counter = 1\n",
    "    new_col = []\n",
    "    for i in range(n_days_in_data): \n",
    "        if old_col[i*n_items] == -1: \n",
    "            new_col += [counter] * n_items\n",
    "            counter += 1\n",
    "        else: \n",
    "            new_col += [0] * n_items\n",
    "            counter = 1\n",
    "    grid_df['days_until_event'] = np.array(new_col)[::-1].astype(np.int16)\n",
    "    \n",
    "    return grid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_cal, df_prices, df_ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@call_parse\n",
    "def fe_base_features(path_data_raw: Param('path to raw data folder', str)='data/raw', \n",
    "                     path_features: Param('path to feature folder', str)='data/features',\n",
    "                     path_to_train_file: Param('path to train data', str)=None) -> None: \n",
    "    \"\"\"Creates the basic categorical, price, and calendar features using \n",
    "    the functions `add_base`, `create_price_fe`, `add_price_fe`, and `add_cal_fe`, \n",
    "    `add_snap_transform_1`, `add_snap_transform_1`, and `add_event_features`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path_data_raw: Param('path to raw data folder', str)='data/raw'\n",
    "    path_features: Param('path to feature folder', str)='data/features'\n",
    "    path_to_train_file: Param('path to train data', str)=None\n",
    "        This is so we can `oos_sales_train_evaluation.csv` file which is \n",
    "        the result of filling zero streaks with NaN where we think an item\n",
    "        is out of stock.\n",
    "    \"\"\"\n",
    "\n",
    "    logging.basicConfig(format='%(asctime)s   %(levelname)s   %(message)s',\n",
    "                        level=logging.DEBUG,\n",
    "                        filename='log.log')\n",
    "    logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))\n",
    "    logging.info('#' * 72)\n",
    "    logging.info('#' * 72)\n",
    "    logging.info('Creating base features')\n",
    "    start = time.time()\n",
    "    \n",
    "    if not path_to_train_file: \n",
    "        path_to_train_file = os.path.join(path_data_raw, 'sales_train_evaluation.csv')\n",
    "    oos_path = os.path.join(os.path.split(path_data_raw)[0], 'interim/oos_sales_train_evaluation.csv')\n",
    "    if os.path.exists(oos_path): \n",
    "        path_to_train_file = oos_path\n",
    "    \n",
    "    # Raw data\n",
    "    df_stv = pd.read_csv(path_to_train_file)\n",
    "    df_prices = pd.read_csv(os.path.join(path_data_raw, 'sell_prices.csv'))\n",
    "    df_cal = pd.read_csv(os.path.join(path_data_raw, 'calendar.csv'))\n",
    "    \n",
    "    # Grid for alignment of all features\n",
    "    grid_df, rec = make_grid_df(path_to_train_file)\n",
    "    original_cols = grid_df.columns\n",
    "    \n",
    "    # Base features\n",
    "    add_base(grid_df, df_stv, rec)\n",
    "    path = os.path.join(path_features, 'fe_base.csv')\n",
    "    save_file(grid_df, path)\n",
    "    del df_stv\n",
    "    grid_df = grid_df[original_cols]\n",
    "    gc.collect()\n",
    "    \n",
    "    # Price features\n",
    "    df_prices = create_price_fe(df_prices)\n",
    "    grid_df = add_price_fe(grid_df, df_prices, df_cal)\n",
    "    del df_prices\n",
    "    gc.collect()\n",
    "    usecols = [c for c in grid_df.columns if c not in original_cols]\n",
    "    path = os.path.join(path_features, 'fe_price.csv')\n",
    "    save_file(grid_df, path, usecols)\n",
    "    grid_df.drop(columns=usecols, inplace=True)\n",
    "    gc.collect()\n",
    "    \n",
    "    # Calendar features\n",
    "    grid_df = add_cal_fe(grid_df, df_cal)\n",
    "    usecols = [c for c in grid_df.columns if c not in original_cols]\n",
    "    path = os.path.join(path_features, 'fe_cal.csv')\n",
    "    save_file(grid_df, path, usecols)\n",
    "    \n",
    "    cols_needed_for_snap = original_cols.tolist() + [\n",
    "        \"event_name_1\", \"event_type_1\", \"event_name_2\", \n",
    "        \"event_type_2\", \"snap_CA\", \"snap_TX\", \"snap_WI\", \"tm_d\"\n",
    "    ]\n",
    "    drop_cols = [c for c in grid_df.columns if c not in cols_needed_for_snap]\n",
    "    grid_df.drop(columns=drop_cols, inplace=True)\n",
    "    gc.collect()\n",
    "    \n",
    "    # Snap and event features\n",
    "    old_cols = grid_df.columns\n",
    "    grid_df = add_snap_transform_1(grid_df)\n",
    "    grid_df = add_snap_transform_2(grid_df)\n",
    "    grid_df = add_event_features(grid_df, n_items=rec.shape[0], n_days_in_data=rec.shape[1])\n",
    "    usecols = [c for c in grid_df.columns if c not in old_cols]\n",
    "    path = os.path.join(path_features, 'fe_snap_event.csv')\n",
    "    save_file(grid_df, path, usecols)\n",
    "    \n",
    "    logging.info(time_taken(start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_base_features(PATH_DATA_RAW, os.path.join(PATH_DATA, 'features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(load_file(f'{os.path.join(PATH_DATA, \"features\")}/fe_base.csv').info())\n",
    "display(load_file(f'{os.path.join(PATH_DATA, \"features\")}/fe_price.csv').info())\n",
    "display(load_file(f'{os.path.join(PATH_DATA, \"features\")}/fe_cal.csv').info())\n",
    "display(load_file(f'{os.path.join(PATH_DATA, \"features\")}/fe_snap_event.csv').info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding features with target statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def encode_target(df: pd.DataFrame, target: str, cols: Union[list, str], func: Union[str, callable], verbose=True):\n",
    "    \"\"\"Uses pandas groupby(col)[target].transform(`func`) to encode \n",
    "    each col in `cols`. The `target` col can be any numerical column.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pd.DataFrame \n",
    "        DataFrame to use\n",
    "    target: str\n",
    "        Name of column to be encoded\n",
    "    cols: Union[list, str] \n",
    "        Name of column or list of colunm groups to be groupbed by. \n",
    "        Each item in the list must work with df.groupby(item), so\n",
    "        multiple column groups should be in a list themselves, such \n",
    "        as `cols` = ['item_id', ['store_id', 'state_id']].\n",
    "    func: callable\n",
    "        Must work with df.groupby(col)[target].transform(func)\n",
    "        \"\"\"\n",
    "    if type(cols) == str: cols = list(cols)\n",
    "    if type(func) == str: func_name = func\n",
    "    else: func_name = func.__name__\n",
    "    for col in cols: \n",
    "        if verbose: print(f'Encoding for {col}')\n",
    "        col_name = '_' + '_'.join(col) + '_'\n",
    "        df[f'enc{col_name}{func_name}'] = df.groupby(col)[target].transform(func).astype(np.float16)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function\n",
    "**Poor encoding**\n",
    "\n",
    "Below is the method I used in the competition. I found that the models performed much better during training, and much worse during validation, so I dropped them from training. \n",
    "\n",
    "I should have done the encoding more carefully, using no future data. I will explore encodings more in the post competition experiments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@call_parse\n",
    "def fe_encodings(path_features: Param('path to feature folder')='data/features', \n",
    "                 path_out_features: Param('path to feature folder for output', str)=None,\n",
    "                 start_test: Param('First day to start nans', int)=1942): \n",
    "    \"\"\"Creates target encoding with mean and std for various columns, with sales after `start_test` \n",
    "    set to np.nan.\n",
    "    \n",
    "    Parameters \n",
    "    ----------\n",
    "    path_features: Param('path to feature folder', str)='data/features'\n",
    "    path_out_features: Param('path to feature folder for output', str)='data/features'\n",
    "        This is mainly to run on kaggle where `path_features` is set to an input dataset,\n",
    "        because we need the rolling window stat features to be present, and \n",
    "        `path_out_features` is set the working directory for the output.\n",
    "    start_test: Param('First day to start nans', int)=1942\n",
    "    \"\"\"\n",
    "    \n",
    "    logging.basicConfig(format='%(asctime)s   %(levelname)s   %(message)s',\n",
    "                        level=logging.DEBUG,\n",
    "                        filename='log.log')\n",
    "    logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))\n",
    "    logging.info('#' * 72)\n",
    "    logging.info('#' * 72)\n",
    "    logging.info('Creating encodings')\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if not path_out_features: \n",
    "        path_out_features = path_features\n",
    "    \n",
    "    grid_df = pd.concat([\n",
    "        load_file(os.path.join(path_features, 'fe_base.csv')), \n",
    "        load_file(os.path.join(path_features, 'fe_cal.csv'), usecols=['tm_m']), \n",
    "        load_file(os.path.join(path_features, 'fe_snap_event.csv'), usecols=['snap_transform_1'])\n",
    "    ], axis=1)\n",
    "    grid_df.loc[grid_df.d >= start_test, 'sales'] = np.nan\n",
    "    original_cols = grid_df.columns    # Saving RAM\n",
    "    grid_df['sales'] = grid_df['sales'].astype(np.float16)\n",
    "    grid_df['snap_transform_1'] = grid_df.snap_transform_1.astype(np.int8)\n",
    "\n",
    "    cols =  [\n",
    "        ['state_id'],\n",
    "        ['store_id'],\n",
    "        ['cat_id'],\n",
    "        ['dept_id'],\n",
    "        ['state_id', 'cat_id'],\n",
    "        ['state_id', 'dept_id'],\n",
    "        ['store_id', 'cat_id'],\n",
    "        ['store_id', 'dept_id'],\n",
    "        ['item_id'],\n",
    "        ['item_id', 'state_id'],\n",
    "        ['item_id', 'store_id']\n",
    "    ]\n",
    "\n",
    "    encode_target(grid_df, 'sales', cols, 'mean')\n",
    "    usecols = [c for c in grid_df.columns if c not in original_cols]\n",
    "    path = os.path.join(path_features, 'fe_enc_mean.csv')\n",
    "    save_file(grid_df, path, usecols)\n",
    "    grid_df.drop(columns=usecols, inplace=True)\n",
    "    gc.collect()\n",
    "\n",
    "    encode_target(grid_df, 'sales', cols, 'std')\n",
    "    usecols = [c for c in grid_df.columns if c not in original_cols]\n",
    "    path = os.path.join(path_features, 'fe_enc_std.csv')\n",
    "    save_file(grid_df, path, usecols)\n",
    "    grid_df.drop(columns=usecols, inplace=True)\n",
    "    gc.collect()\n",
    "\n",
    "    cols = [\n",
    "        ['store_id', 'dept_id', 'snap_transform_1'],\n",
    "        ['item_id', 'store_id', 'snap_transform_1'],\n",
    "        ['store_id', 'dept_id', 'tm_m'],\n",
    "        ['item_id', 'store_id', 'tm_m'],\n",
    "        ['store_id', 'dept_id', 'snap_transform_1', 'tm_m'],\n",
    "        ['item_id', 'store_id', 'snap_transform_1', 'tm_m'],\n",
    "    ]\n",
    "\n",
    "    encode_target(grid_df, 'sales', cols, 'mean')\n",
    "    encode_target(grid_df, 'sales', cols, 'std')\n",
    "    usecols = [c for c in grid_df.columns if c not in original_cols]\n",
    "    path = os.path.join(path_features, 'fe_enc_special.csv')\n",
    "    save_file(grid_df, path, usecols)\n",
    "    \n",
    "    logging.info(time_taken(start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_encodings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(load_file(f'{PATH_DATA_FEATURES}/fe_enc_mean.csv').info())\n",
    "display(load_file(f'{PATH_DATA_FEATURES}/fe_enc_std.csv').info())\n",
    "display(load_file(f'{PATH_DATA_FEATURES}/fe_enc_special.csv').info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lags and rolling features\n",
    "> These are the features created from the raw sales data directly. \n",
    "\n",
    "**Be carefule with lag features.**\n",
    "We must be mindful that we will not have the same information for all days in forecast horizon. If we want a single model to predict all days, we can only use lagging features from 28 days and older. In order to create one set of features that we can use for all predictions we will do the following: \n",
    "* Create all lagging features as if we are building a model for 1 day into the future. So \"lag_1\" means sales one day before the first day of the prediction horizon. \n",
    "* When building a model for the nth day of the horizon, we need to shift the lag features n - 1 extra days. Since we have 30490 time series, We do this by shifting the index of the features by (n - 1) * 30490 so that the lag features for all training and testing data will be lagged by an extra (n - 1) days to keep information aligned properly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic lag features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before reshaping the data to become a column, \n",
    "    we need to shift our rectangle `lag_shift`\n",
    "    by prepended the data with np.nans\n",
    "    to make up for the data we have cut off.\n",
    "    Therefore, all the d_1 products \n",
    "    in grid_df will have np.nan for lag_1. In \n",
    "    fact, as we carry out this process for all \n",
    "    lag days, rows with sales on d_x will have \n",
    "    np.nan values for all lags lag_y where y >= x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df, rec = make_grid_df(os.path.join(PATH_DATA_RAW, 'sales_train_evaluation.csv'))\n",
    "grid_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def make_lag_col(rec: np.array, lag: int) -> np.array: \n",
    "    \"\"\"Transform the 'rectangle' of time series into a lag feature\"\"\"\n",
    "    lag_rec = np.roll(rec, shift=lag, axis=1).astype(np.float16)\n",
    "    lag_rec[:, :lag] = np.nan\n",
    "    return lag_rec.T.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec[:2, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_lag_col(rec[:2, :5], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def add_lags(grid_df, rec, lags=range(1,16)):\n",
    "    start_time = time.time()\n",
    "    logging.info( 72 * '#')\n",
    "    logging.info('\\nAdding lag columns')\n",
    "    for lag in lags:\n",
    "        grid_df[f'lag_{lag}'] = make_lag_col(rec, lag)\n",
    "    logging.info(time_taken(start_time))\n",
    "    return grid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "add_lags(grid_df, rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas shift \n",
    "We can also \n",
    "just use pandas shift which is easier to implement and not much slower. \n",
    "The only downside is that we must use `num_series`\n",
    "to shift by the correct increment. \n",
    "Here we will do it for g, which was the\n",
    "same as grid_df before adding lags.\n",
    "Our time was not wasted though, \n",
    "because we learned skills that we will \n",
    "need for making rolling windows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g, rec_tmp = make_grid_df(os.path.join(PATH_DATA_RAW, 'sales_train_evaluation.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "num_series = df_stv.shape[0]\n",
    "for i in range(1,16):\n",
    "    g[f'lag_{i}'] = g['sales'].shift(num_series * i).astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del g\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@call_parse\n",
    "def fe_lags(path_data_raw: Param('path to raw data folder', str)='data/raw', \n",
    "            path_features: Param('path to feature folder', str)='data/features',\n",
    "            path_to_train_file: Param('path to train data', str)=None) -> None:  \n",
    "    \"\"\"Creates lags and rolling window features using `add_lags` `add_rolling_cols`\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path_data_raw: Param('path to raw data folder', str)='data/raw'\n",
    "    path_features: Param('path to feature folder', str)='data/features'\n",
    "    path_to_train_file: Param('path to train data', str)=None\n",
    "        This is so we can `oos_sales_train_evaluation.csv` file which is \n",
    "        the result of filling zero streaks with NaN where we think an item\n",
    "        is out of stock.\n",
    "    \"\"\"\n",
    "    \n",
    "    logging.basicConfig(format='%(asctime)s   %(levelname)s   %(message)s',\n",
    "                        level=logging.DEBUG,\n",
    "                        filename='log.log')\n",
    "    logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))\n",
    "    logging.info('#' * 72)\n",
    "    logging.info('#' * 72)\n",
    "    logging.info('Making lags and rolling window features')\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if not path_to_train_file: \n",
    "        path_to_train_file = os.path.join(path_data_raw, 'sales_train_evaluation.csv')\n",
    "    oos_path = os.path.join(os.path.split(path_data_raw)[0], 'interim/oos_sales_train_evaluation.csv')\n",
    "    if os.path.exists(oos_path): \n",
    "        path_to_train_file = oos_path\n",
    "    \n",
    "    grid_df, rec = make_grid_df(path_to_train_file)\n",
    "    original_cols = grid_df.columns\n",
    "    \n",
    "    # Lags\n",
    "    max_lag = 84\n",
    "    cols_per_file = 14\n",
    "    for lag in range(1, max_lag, cols_per_file): \n",
    "        last_lag = lag + cols_per_file\n",
    "        add_lags(grid_df, rec, lags=range(lag, last_lag))\n",
    "        usecols = [c for c in grid_df.columns if c not in original_cols]\n",
    "        path = os.path.join(path_features, f'shift_fe_lags_{lag}_{last_lag - 1}.csv')\n",
    "        save_file(grid_df, path, usecols)\n",
    "        grid_df.drop(columns=usecols, inplace=True)\n",
    "        gc.collect()\n",
    "    \n",
    "    logging.info(time_taken(start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_lags()\n",
    "# fe_lags(PATH_DATA_RAW, path_features='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lag = 84\n",
    "cols_per_file = 14\n",
    "for lag in range(1, max_lag, cols_per_file):\n",
    "    display(load_file(f'{PATH_DATA_FEATURES}/shift_fe_lags_{lag}_{lag + cols_per_file - 1}.csv').info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rolling features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rolling window function\n",
    "Please check out \n",
    "[\"Efficient rolling statistics with NumPy\"](https://rigtorp.se/2011/01/01/rolling-statistics-numpy.html)\n",
    "by [Erik Rigtorp](https://rigtorp.se/). \n",
    "The article shows some cool numpy tricks to do really fast rolling window calculations by creating \"rolling windows views\"\n",
    "\n",
    "**Update to the article 2021-04-21:**\n",
    "\"NumPy now comes with a builtin function sliding_window_view that does exactly this. There’s also the Bottleneck library with optimized functions for rolling mean, standard deviation etc.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def rolling_window(a: np.array, window: int):\n",
    "    \"\"\"\n",
    "    A super fast way of getting rolling window view with size `window` on a numpy array. \n",
    "    Reference: https://rigtorp.se/2011/01/01/rolling-statistics-numpy.html\n",
    "    \"\"\"\n",
    "    \n",
    "    shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n",
    "    strides = a.strides + (a.strides[-1],)\n",
    "    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(10).reshape((2,5))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rw = rolling_window(x, 3)\n",
    "rw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(rw, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(rw, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(rw, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del x, rw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def split_array(ary, sections, axis=0):\n",
    "    \"\"\"Works just like np.split, but sections must be a \n",
    "    single integer. It will work, even when sections doesn't \n",
    "    evenly divide the length of ary.\n",
    "    \n",
    "    This avoids errors that occur when using \n",
    "    `make_rolling_col` with high `n_splits` that do not \n",
    "    divide the number of series evenly.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> x = np.arange(9)\n",
    "    >>> split_array(x, 4)\n",
    "    [array([0, 1, 2]), array([3, 4, 5]), array([6, 7, 8])]\n",
    "\n",
    "    >>> split_array(x, 5)\n",
    "    [array([0, 1]), array([2, 3]), array([4, 5]), array([6, 7, 8])]\"\"\"\n",
    "    w = int(np.ceil(len(ary)/sections))\n",
    "    return np.split(ary, [w * i for i in range(1, int(np.ceil(len(ary)//w)))], axis=axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(range(9))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_array(x, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def make_rolling_col(rw, function, n_splits=10): \n",
    "    \"\"\"Returns a one dimensional np.array after `function` has been applied to the rolling window view `rw`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    rw: a rolling window view as defined in this same module\n",
    "    function: a callable such as np.mean  \n",
    "        Axis must be the second parameter.\n",
    "    n_splits: int\n",
    "        Must divide rw.shape[0]. This will avoid a RAM overflow \n",
    "        that can occur when using \n",
    "    \n",
    "    We need to take off the last columns to\n",
    "    get the rolling feature shifted one day, \n",
    "    since we are making features for training. \n",
    "    For predicting day 10, we only know the \n",
    "    rolling features up to day 9.\n",
    "    \"\"\"\n",
    "    \n",
    "    split_rw = split_array(rw, n_splits, axis=0)\n",
    "    split_col = [function(rw, -1) for rw in split_rw]\n",
    "    col = np.concatenate(split_col)\n",
    "    col = col[:, :-1].T.reshape(-1,)\n",
    "\n",
    "    # The new column must be prepended with np.nans to account for missing gaps\n",
    "    nans = np.zeros(rw.shape[-1] * rw.shape[0]) + np.nan\n",
    "    return np.append(nans, col).astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rw = rolling_window(rec, 3)\n",
    "print(f'The shape {rw.shape}, represents (num_series, num_windows, window_size)')\n",
    "function = np.mean\n",
    "col = make_rolling_col(rw, function)\n",
    "\n",
    "print('Make sure the shape of the resulting column matches grid_df')\n",
    "print(grid_df.shape[0],'=',  col.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(10).reshape((2,5))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rw = rolling_window(x, 3)\n",
    "make_rolling_col(rw, function, n_splits=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del rw, function, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "###### Code that shows why we need `n_splits` in `make_rolling_col`\n",
    "################ make rolling col helper ###############\n",
    "\n",
    "#### version 1 ######\n",
    "\n",
    "# def make_rolling_col_v1(rw, window, function): \n",
    "#     # We need to take off the last columns to\n",
    "#     # get the rolling feature shifted one day. \n",
    "\n",
    "#     col = function(rw, -1)[:, :-1].T.reshape(-1,)\n",
    "\n",
    "#     # The new column must be prepended with np.nans \n",
    "#     # to account for missing gaps\n",
    "\n",
    "#     return np.append(np.zeros(30490 * window) + np.nan, col).astype(np.float16)\n",
    "\n",
    "# This version is commented out because it breaks my \n",
    "# notebook session. I get a message saying I have tried \n",
    "# to allocate too much memory. I discovered that the \n",
    "# problem was with np.std when the window was 30 or \n",
    "# above. I believe the problem was np was trying to \n",
    "# calculate std for all windows, and that was just \n",
    "# too much. But I experimented with np.split(rw), and \n",
    "# found that there was no problem calculating std in \n",
    "# 10 batches, even for window 180. I have set splits \n",
    "# to 10. If you have a function or window that still \n",
    "# causes a crash, you can increase splits to 3049, the \n",
    "# next factor of 30490. \n",
    "# I have noticed a slight slow down \n",
    "# when doing this, so I will leave it at 10 for now. \n",
    "\n",
    "##### experiment code to show problem #####\n",
    "## This will break \n",
    "# rw = rolling_window(rec, 180)\n",
    "# np.std(rw, -1) \n",
    "\n",
    "## This will not break\n",
    "# rw = rolling_window(rec, 180)\n",
    "# x= np.split(rw, 10, axis=0)\n",
    "# x = [np.std(rw, -1) for rw in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some more functions for rolling windows \n",
    "These functions are designed to act on a rolling_window array\n",
    "created by the rolling_window function, similar to np.mean\n",
    "or np.std. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def diff_mean(rolling_window, axis=-1): \n",
    "    \"\"\"For M5 purposes, used on an object generated by the \n",
    "    rolling_window function. Returns the mean of the first \n",
    "    difference of a window of sales.\"\"\"\n",
    "    return np.diff(rolling_window, axis=axis).mean(axis=axis)\n",
    "\n",
    "def diff_nanmean(rolling_window, axis=-1): \n",
    "    \"\"\"For M5 purposes, used on an object generated by the \n",
    "    rolling_window function. Returns the mean of the first \n",
    "    difference of a window of sales.\"\"\"\n",
    "    return np.nanmean(np.diff(rolling_window, axis=axis), axis = axis)\n",
    "\n",
    "def mean_decay(rolling_window, axis=-1): \n",
    "    \"\"\"Returns the mean_decay along an axis of a rolling window object, \n",
    "    which is created by the rolling_window() function.\"\"\"\n",
    "    \n",
    "    # decay window must be as long as the last \n",
    "    # dimension in the rolling window\n",
    "    decay_window = np.power(.9, np.arange(rolling_window.shape[-1]))[::-1]\n",
    "    decay_sum = decay_window.sum()\n",
    "    return (rolling_window * decay_window).sum(axis = -1) / decay_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def add_rolling_cols(\n",
    "    grid_df: pd.DataFrame,\n",
    "    rec: np.array,\n",
    "    windows: list,\n",
    "    functions: list,\n",
    "    function_names: list=None, \n",
    "    n_splits: \"list or int\"=10, \n",
    "): \n",
    "    \"\"\"Adds rolling features to grid_df.\"\"\"\n",
    "    \n",
    "    logging.info(72 * '#')\n",
    "    logging.info('Adding rolling columns')\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if not function_names: function_names = [f.__name__ for f in functions]\n",
    "    if not n_splits: n_splits = [1] * len(functions)\n",
    "    if type(n_splits) == int: n_splits = [n_splits] * len(functions)\n",
    "    zipped = list(zip(functions, function_names, n_splits))\n",
    "    \n",
    "    for window in windows: \n",
    "        rw = rolling_window(rec, window)\n",
    "        for func, f_name, n_n_splits in zipped: \n",
    "            s_time = time.time()\n",
    "            grid_df[f'shift_1_rolling_{f_name}_{str(window)}'] = make_rolling_col(rw, func, n_n_splits)\n",
    "            logging.info(f'{f_name} with window {window}')\n",
    "            logging.info(time_taken(s_time))\n",
    "            \n",
    "    logging.info(f'Rolling cols total time:') \n",
    "    logging.info(time_taken(start_time))\n",
    "    return grid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df = add_rolling_cols(grid_df, \n",
    "                 rec, \n",
    "                 windows=[7, 14, 30, 60, 140], \n",
    "                 functions=[np.mean, np.std], \n",
    "                 function_names=['mean', 'std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export    \n",
    "@call_parse\n",
    "def fe_rw_stats(path_data_raw: Param('path to raw data folder', str)='data/raw', \n",
    "                path_features: Param('path to feature folder', str)='data/features',\n",
    "                path_to_train_file: Param('path to train data', str)=None) -> None:  \n",
    "    \"\"\"Creates lags and rolling window features using `add_lags` `add_rolling_cols`\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path_data_raw: Param('path to raw data folder', str)='data/raw'\n",
    "    path_features: Param('path to feature folder', str)='data/features'\n",
    "    path_to_train_file: Param('path to train data', str)=None\n",
    "        This is so we can `oos_sales_train_evaluation.csv` file which is \n",
    "        the result of filling zero streaks with NaN where we think an item\n",
    "        is out of stock.\n",
    "    \"\"\"\n",
    "    \n",
    "    logging.basicConfig(format='%(asctime)s   %(levelname)s   %(message)s',\n",
    "                        level=logging.DEBUG,\n",
    "                        filename='log.log')\n",
    "    logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))\n",
    "    logging.info('#' * 72)\n",
    "    logging.info('#' * 72)\n",
    "    logging.info('Making lags and rolling window features')\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if not path_to_train_file: \n",
    "        path_to_train_file = os.path.join(path_data_raw, 'sales_train_evaluation.csv')\n",
    "    oos_path = os.path.join(os.path.split(path_data_raw)[0], 'interim/oos_sales_train_evaluation.csv')\n",
    "    if os.path.exists(oos_path): \n",
    "        path_to_train_file = oos_path\n",
    "    \n",
    "    grid_df, rec = make_grid_df(path_to_train_file)\n",
    "    original_cols = grid_df.columns\n",
    "    \n",
    "    # Rolling window stats\n",
    "    grid_df = add_rolling_cols(\n",
    "        grid_df, \n",
    "        rec, \n",
    "        windows=[3, 7], \n",
    "        functions=[np.nanmean, np.nanmedian, mean_decay, diff_nanmean, np.nanmin, np.nanmax, np.nanstd]\n",
    "    )\n",
    "    usecols = [c for c in grid_df.columns if c not in original_cols]\n",
    "    path = os.path.join(path_features, 'shift_fe_rw_1.csv')\n",
    "    save_file(grid_df, path, usecols)\n",
    "    grid_df.drop(columns=usecols, inplace=True)\n",
    "    gc.collect()\n",
    "    \n",
    "    grid_df = add_rolling_cols(\n",
    "        grid_df, \n",
    "        rec, \n",
    "        windows=[14, 30], \n",
    "        functions=[np.nanmean, np.nanmedian, mean_decay, diff_nanmean, np.nanmin, np.nanmax, np.nanstd], \n",
    "        n_splits=10\n",
    "    )\n",
    "    usecols = [c for c in grid_df.columns if c not in original_cols]\n",
    "    path = os.path.join(path_features, 'shift_fe_rw_2.csv')\n",
    "    save_file(grid_df, path, usecols)\n",
    "    grid_df.drop(columns=usecols, inplace=True)\n",
    "    gc.collect()\n",
    "    \n",
    "    grid_df = add_rolling_cols(\n",
    "        grid_df, \n",
    "        rec, \n",
    "        windows=[60, 140], \n",
    "        functions=[np.nanmean, np.nanmedian, mean_decay, diff_nanmean, np.nanmin, np.nanmax, np.nanstd], \n",
    "        n_splits=50\n",
    "    )\n",
    "    usecols = [c for c in grid_df.columns if c not in original_cols]\n",
    "    path = os.path.join(path_features, 'shift_fe_rw_3.csv')\n",
    "    save_file(grid_df, path, usecols)\n",
    "    \n",
    "    logging.info(time_taken(start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_rw_stats()\n",
    "# fe_rw_stats(PATH_DATA_RAW, path_features='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(load_file(f'{PATH_DATA_FEATURES}/shift_fe_rw_1.csv').info())\n",
    "display(load_file(f'{PATH_DATA_FEATURES}/shift_fe_rw_2.csv').info())\n",
    "display(load_file(f'{PATH_DATA_FEATURES}/shift_fe_rw_3.csv').info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average for each day of the week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature telling how long its been since theres been a sale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_days_since_sale(grid_df, num_series=30490): \n",
    "    \"\"\"Returns a column that shows how many days its been\n",
    "    Since there has been a sale.\"\"\"\n",
    "    df = grid_df[['id', 'd', 'sales']]\n",
    "    df['non-zero'] = df['sales'] > 0\n",
    "    df['cum_sum'] = df.groupby(['id'])['non-zero'].transform(np.cumsum)\n",
    "    tmp_df = df[['id', 'd', 'cum_sum']]\n",
    "    tmp_df.drop_duplicates(['id', 'cum_sum'], inplace=True)\n",
    "    tmp_df.columns = ['id', 'd_min', 'cum_sum']\n",
    "    df = df.merge(tmp_df, on=['id', 'cum_sum'], how='left')\n",
    "    df['days_since_sale'] = df['d'] - df['d_min']\n",
    "\n",
    "    # We will only know this stat from one day before \n",
    "    # so we need to shift the column forward by one day.\n",
    "    df['days_since_sale'] = df['days_since_sale'].shift(num_series)\n",
    "\n",
    "    return df['days_since_sale'].astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def add_dow_means(grid_df, rec, n_weeks) -> None: \n",
    "    \"\"\"Adds features to grid_df for the mean of each day of the week\n",
    "    for the past `n_weeks`.\n",
    "    \n",
    "    For any row, the column 'mean_{n_weeks}_dow_{i}' represents the mean \n",
    "    of the last `n_weeks` of sales for the day of the week that is `i` \n",
    "    days behind the date of this row. So if today is Friday, n_weeks=4\n",
    "    and i = 1, this column is equal to the mean sales of the last 4\n",
    "    Thursdays.\"\"\"\n",
    "    for i in range(7):\n",
    "        days = [d for d in range(1,n_weeks * 7 + 1) if d%7 == i]\n",
    "        add_lags(grid_df, rec, days)\n",
    "        dow_cols = [f'lag_{d}' for d in days]\n",
    "        grid_df[f'mean_{n_weeks}_dow_{i}'] = grid_df[dow_cols].mean(axis=1).astype(np.float16)\n",
    "        grid_df.drop(dow_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@call_parse\n",
    "def fe_dow_means(path_data_raw: Param('path to raw data folder', str)='data/raw', \n",
    "                 path_features: Param('path to feature folder', str)='data/features',\n",
    "                 path_to_train_file: Param('path to train data', str)=None) -> None: \n",
    "    \"\"\"\n",
    "    Creates the features for day of week means using `add_dow_means`\n",
    "    \n",
    "    We also use 'get_days_since_sale' in this script since there isn't \n",
    "    another group very similar to this feature.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path_data_raw: Param('path to raw data folder', str)='data/raw'\n",
    "    path_features: Param('path to feature folder', str)='data/features'\n",
    "    path_to_train_file: Param('path to train data', str)=None\n",
    "        This is so we can `oos_sales_train_evaluation.csv` file which is \n",
    "        the result of filling zero streaks with NaN where we think an item\n",
    "        is out of stock.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    logging.basicConfig(format='%(asctime)s   %(levelname)s   %(message)s',\n",
    "                        level=logging.DEBUG,\n",
    "                        filename='log.log')\n",
    "    logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))\n",
    "    logging.info('#' * 72)\n",
    "    logging.info('#' * 72)\n",
    "    logging.info('Making day of week means features')\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if not path_to_train_file: \n",
    "        path_to_train_file = os.path.join(path_data_raw, 'sales_train_evaluation.csv')\n",
    "    oos_path = os.path.join(os.path.split(path_data_raw)[0], 'interim/oos_sales_train_evaluation.csv')\n",
    "    if os.path.exists(oos_path): \n",
    "        path_to_train_file = oos_path\n",
    "    grid_df, rec = make_grid_df(path_to_train_file)\n",
    "    original_cols = grid_df.columns\n",
    "\n",
    "    ######################## Mean day of week ############################\n",
    "    add_dow_means(grid_df, rec, 4)\n",
    "    add_dow_means(grid_df, rec, 20)\n",
    "\n",
    "    ######################### Last sale day ##############################\n",
    "    grid_df['days_since_sale'] = get_days_since_sale(grid_df, num_series=rec.shape[0])\n",
    "\n",
    "    usecols = [c for c in grid_df.columns if c not in original_cols]\n",
    "    path = os.path.join(path_features, 'shift_fe_dow_means_and_days_since_sale.csv')\n",
    "    save_file(grid_df, path, usecols)\n",
    "    grid_df.drop(columns=usecols, inplace=True)\n",
    "    gc.collect()\n",
    "    \n",
    "    logging.info(time_taken(start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fe_dow_means(PATH_DATA_RAW, '.')\\\n",
    "fe_dow_means()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(load_file(f'{PATH_DATA_FEATURES}/shift_fe_dow_means_and_days_since_sale.csv').info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shifted lag rolling features \n",
    "Perhaps I want to also want to know 7 day rolling \n",
    "mean, but from 7 seven days ago. This could go \n",
    "directly into a model, or we could create a weekly\n",
    "`momentum_7_rolling_mean_7 = shift_1_rolling_mean_7/shift_8_rolling_mean_7`. \n",
    "We have already calculated these features, \n",
    "we just need to shift the columns by `num_series * (shift_days - 1)`.\n",
    "We subtract 1 from shift_days because the column shift_1_rolling_mean_7\n",
    "is already shifted 1 day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def add_shift_cols(grid_df: pd.DataFrame, shifts: list, cols: list, num_series: int=30490, momentum: bool=True): \n",
    "    \"\"\"Adds shift_{`shift`} and momentum_{`shift` - 1} features for each \n",
    "    int `shift` in `shifts` for each column in `cols`. `cols` must be \n",
    "    a list of columns that begin with 'shift_1' for this function to work.\n",
    "    \"\"\"\n",
    "    \n",
    "    for shift_val in shifts: \n",
    "        for col in cols: \n",
    "            shift_name = f\"{col.replace('shift_1', f'shift_{shift_val}')}\"\n",
    "            grid_df[shift_name] = grid_df[col].shift((shift_val - 1) * num_series)\n",
    "            if momentum: \n",
    "                mom_name = col.replace('shift_1', f'momentum_{shift_val - 1}')\n",
    "                grid_df[mom_name] = grid_df[col] / grid_df[shift_name]\n",
    "    return grid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Adding shifted rolling mean ###############\n",
    "shifts = [8, 15, 22, 29]\n",
    "cols = [f'shift_1_rolling_mean_{i}' for i in [7, 14]]\n",
    "add_shift_cols(grid_df, shifts, cols, num_series=df_stv.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@call_parse \n",
    "def fe_shifts_momentum(path_features: Param('path to feature folder', str)='data/features',\n",
    "                       path_out_features: Param('path to feature folder for output', str)='',\n",
    "                       num_series: Param('Number of series for shifting', int)=30490) -> None: \n",
    "    \"\"\"Creates shifts and momentum features using `add_shift_cols`\\n\n",
    "    \n",
    "    Parameters\\n\n",
    "    ----------\\n\n",
    "    path_features: Param('path to feature folder', str)='data/features'\\n\n",
    "    path_out_features: Param('path to feature folder for output', str)='data/features'\n",
    "        This is mainly to run on kaggle where `path_features` is set to an input dataset,\n",
    "        because we need the rolling window stat features to be present, and \n",
    "        `path_out_features` is set the working directory for the output.\\n\n",
    "    num_series: Param('Number of series for shifting', int)=30490\n",
    "    \"\"\"\n",
    "\n",
    "    logging.basicConfig(format='%(asctime)s   %(levelname)s   %(message)s',\n",
    "                        level=logging.DEBUG,\n",
    "                        filename='log.log')\n",
    "    logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))\n",
    "    logging.info('#' * 72)\n",
    "    logging.info('#' * 72)\n",
    "    logging.info('Making shifts and momentum features')\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if not path_out_features: \n",
    "        path_out_features = path_features\n",
    "    dict_features = get_file_cols_dict(path_features)\n",
    "\n",
    "    shifts = [8, 29]\n",
    "    cols = [\"shift_1_rolling_nanmean_7\",\n",
    "            \"shift_1_rolling_mean_decay_7\",\n",
    "            \"shift_1_rolling_diff_nanmean_7\",]\n",
    "    grid_df = load_features(path_features=path_features, dict_features=dict_features, features=cols)\n",
    "    original_cols = grid_df.columns\n",
    "    grid_df = add_shift_cols(grid_df, shifts, cols, num_series)\n",
    "    \n",
    "    usecols = [c for c in grid_df.columns if c not in original_cols]\n",
    "    path = os.path.join(path_out_features, 'shift_fe_shifts_mom_1.csv')\n",
    "    save_file(grid_df, path, usecols)\n",
    "    grid_df.drop(columns=usecols, inplace=True)\n",
    "    gc.collect()\n",
    "    \n",
    "    cols = [\"shift_1_rolling_nanmean_30\",\n",
    "            \"shift_1_rolling_mean_decay_30\",\n",
    "            \"shift_1_rolling_diff_nanmean_30\",]\n",
    "    grid_df = load_features(path_features=path_features, dict_features=dict_features, features=cols)\n",
    "    original_cols = grid_df.columns\n",
    "    grid_df = add_shift_cols(grid_df, shifts, cols, num_series)\n",
    "    \n",
    "    usecols = [c for c in grid_df.columns if c not in original_cols]\n",
    "    path = os.path.join(path_out_features, 'shift_fe_shifts_mom_2.csv')\n",
    "    save_file(grid_df, path, usecols)\n",
    "    grid_df.drop(columns=usecols, inplace=True)\n",
    "    gc.collect()\n",
    "\n",
    "    shifts = [29, 91]\n",
    "    cols = [\"shift_1_rolling_nanmean_60\",\n",
    "            \"shift_1_rolling_mean_decay_60\",\n",
    "            \"shift_1_rolling_diff_nanmean_60\",]\n",
    "    grid_df = load_features(path_features=path_features, dict_features=dict_features, features=cols)\n",
    "    original_cols = grid_df.columns\n",
    "    grid_df = add_shift_cols(grid_df, shifts, cols, num_series)\n",
    "    \n",
    "    usecols = [c for c in grid_df.columns if c not in original_cols]\n",
    "    path = os.path.join(path_out_features, 'shift_fe_shifts_mom_3.csv')\n",
    "    save_file(grid_df, path, usecols)\n",
    "    grid_df.drop(columns=usecols, inplace=True)\n",
    "    gc.collect()\n",
    "\n",
    "    logging.info(72 * '#')\n",
    "    logging.info(time_taken(start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fe_shifts_momentum('.', num_series=df_stv.shape[0])\n",
    "# time.sleep(1)\n",
    "fe_shifts_momentum(num_series=df_stv.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(load_file(f'{PATH_DATA_FEATURES}/shift_fe_shifts_mom_1.csv').info())\n",
    "display(load_file(f'{PATH_DATA_FEATURES}/shift_fe_shifts_mom_2.csv').info())\n",
    "display(load_file(f'{PATH_DATA_FEATURES}/shift_fe_shifts_mom_3.csv').info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction of lags \n",
    "Since we have so many lags, I will try to use pca to reduce the number of features I have. \n",
    "I can't fit all the lags into memory, so I will create the pca features iteratively, \n",
    "starting with lags 71 through 84, save the file, then save the top 7 components \n",
    "to do pca again with lags 57 through 70, and so on until I have 14 pca components \n",
    "for lags 1 through 84. Then I can decide how many lags features I want to keep without\n",
    "reducing their dimension. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@call_parse\n",
    "def fe_ipca_lags(path_data_raw: Param('path to raw data folder', str)='data/raw',\n",
    "                 path_features: Param('Path to feature file', str)='data/features',\n",
    "                 path_to_train_file: Param('path to train data', str)=None,\n",
    "                 end: Param('last day to start lags from', int)=1,\n",
    "                 restart: Param('start if resuming with lags_df.pkl in restart_dir', int)=None,\n",
    "                 target: Param('Name of target column', str)='sales') -> None: \n",
    "    \"\"\"Creates ipca columns for 84 lag days, starting from the end\n",
    "    and accumulating backward. With 16 GB of RAM, we can only fit 14\n",
    "    with ipca at a time, so for each iteration, we: \n",
    "    \n",
    "        1) Create 14 new lag days, and use ipca to reduce it to \n",
    "        the top 7 compnents.\n",
    "        2) Combine those with the top 7 components from the previous step\n",
    "        3) Perform ipca on these 14, features, save the output, and \n",
    "        keep the top 7 components for the next iteration.\n",
    "        \n",
    "    In the end we will have files with the top 14 ipca components \n",
    "    for each of these separate ranges: \n",
    "    Days 1_84\n",
    "    Days 15_84\n",
    "    Days 29_84\n",
    "    Days 43_84\n",
    "    Days 57_84\n",
    "    Days 71_84\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path_data_raw: Param('path to raw data folder', str)='data/raw',\n",
    "    path_features: Param('path to feature folder', str)='data/features'\n",
    "    path_to_train_file: Param('path to train data', str)=None\n",
    "        This is so we can use `oos_sales_train_evaluation.csv` file which is \n",
    "        the result of filling zero streaks with NaN where we think an item\n",
    "        is out of stock.\n",
    "    target: Param('Name of target column', str)='sales'\n",
    "    \"\"\"\n",
    "    \n",
    "    logging.basicConfig(format='%(asctime)s   %(levelname)s   %(message)s',\n",
    "                        level=logging.DEBUG,\n",
    "                        filename='log.log')\n",
    "    logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))\n",
    "    logging.info('#' * 72)\n",
    "    logging.info('#' * 72)\n",
    "    logging.info('Using ipca to reduce lag feature dimensions')\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if not path_to_train_file: \n",
    "        path_to_train_file = os.path.join(path_data_raw, 'sales_train_evaluation.csv')\n",
    "    oos_path = os.path.join(os.path.split(path_data_raw)[0], 'interim/oos_sales_train_evaluation.csv')\n",
    "    if os.path.exists(oos_path): \n",
    "        path_to_train_file = oos_path\n",
    "\n",
    "    grid_df, rec = make_grid_df(path_to_train_file, 1)\n",
    "\n",
    "    ############## Start with indexes with non null sales #############\n",
    "    # Make sure the first test day is not null, \n",
    "    # since lags will not be null, we need these\n",
    "    # indexes to be present. \n",
    "    start_test = grid_df.d.tolist()[-1] \n",
    "    grid_df.loc[grid_df.d == start_test, target] = 0\n",
    "    lags_index = grid_df[grid_df.sales.notnull()].index\n",
    "    full_index = grid_df[[]]\n",
    "    del grid_df\n",
    "    gc.collect()\n",
    "    \n",
    "    start = restart if restart else 71\n",
    "    for start_range in range(start, end - 1, -14):\n",
    "        logging.info(f'ipca for {start_range}_84')\n",
    "        \n",
    "        lags_df = full_index.copy()\n",
    "        lags_df = add_lags(\n",
    "            lags_df, rec, range(start_range, start_range + 14)\n",
    "        ).reindex(lags_index).fillna(0).astype('float32')\n",
    "        lags_df[lags_df.columns] = StandardScaler(copy=False).fit_transform(lags_df.values)\n",
    "        gc.collect()\n",
    "\n",
    "        ipca = IncrementalPCA(n_components=14, copy=False)\n",
    "        cols = lags_df.columns[:ipca.n_components]\n",
    "        lags_df[cols] = ipca.fit_transform(lags_df.values).astype(np.float16)\n",
    "        logging.info(f'ipca_{start_range}_{start_range + 13} explained variance ratio:')\n",
    "        logging.info(ipca.explained_variance_ratio_.round(3))\n",
    "\n",
    "        if start_range == 71: \n",
    "            name = f'ipca_{start_range}_84'\n",
    "            lags_df.columns = [name + f'_comp_{i}' for i in range(1, 15)]\n",
    "            path = os.path.join(path_features, 'shift_fe_' + name + '.csv')\n",
    "            save_file(lags_df, path, save_index=True)\n",
    "            lags_df.iloc[:, :7].to_pickle('lags_df.pkl')\n",
    "            gc.collect()\n",
    "            continue\n",
    "        \n",
    "        # Combine top 7 components of new lags pca with top 7 components from the last iteration. \n",
    "        lags_df = lags_df.iloc[:, :7]\n",
    "        lags_df = pd.concat([lags_df, pd.read_pickle('lags_df.pkl')], axis=1).astype('float32')\n",
    "        gc.collect()\n",
    "\n",
    "        ipca = IncrementalPCA(n_components=14, copy=False)\n",
    "        cols = lags_df.columns[:ipca.n_components]\n",
    "        lags_df[cols] = ipca.fit_transform(lags_df.values).astype(np.float16)\n",
    "        logging.info(f'ipca_{start_range}_84 ipca of ipca cols explained variance ratio:')\n",
    "        logging.info(ipca.explained_variance_ratio_.round(3))\n",
    "        \n",
    "        # Save all components as csv and top 7 as pickle for next iteration.\n",
    "        name = f'ipca_{start_range}_84'\n",
    "        lags_df.columns = [name + f'_comp_{i}' for i in range(1, 15)]\n",
    "        path = os.path.join(path_features, 'shift_fe_' + name + '.csv')\n",
    "        save_file(lags_df, path, save_index=True)\n",
    "        lags_df.iloc[:, :7].to_pickle('lags_df.pkl')\n",
    "        del lags_df\n",
    "        gc.collect()\n",
    "        \n",
    "    os.remove('lags_df.pkl')\n",
    "        \n",
    "    logging.info(72 * '#')\n",
    "    logging.info(time_taken(start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fe_ipca_lags(PATH_DATA_RAW, '.')\n",
    "fe_ipca_lags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(load_file(f'{PATH_DATA_FEATURES}/shift_fe_ipca_1_84.csv').info())\n",
    "display(load_file(f'{PATH_DATA_FEATURES}/shift_fe_ipca_15_84.csv').info())\n",
    "display(load_file(f'{PATH_DATA_FEATURES}/shift_fe_ipca_29_84.csv').info())\n",
    "display(load_file(f'{PATH_DATA_FEATURES}/shift_fe_ipca_43_84.csv').info())\n",
    "display(load_file(f'{PATH_DATA_FEATURES}/shift_fe_ipca_57_84.csv').info())\n",
    "display(load_file(f'{PATH_DATA_FEATURES}/shift_fe_ipca_71_84.csv').info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets see all the features we created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_file_cols_dict(PATH_DATA_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm data/features/*csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@call_parse\n",
    "def fe(): \n",
    "    fe_base_features()\n",
    "    fe_lags()\n",
    "    fe_rw_stats()\n",
    "    fe_dow_means()\n",
    "    fe_shifts_momentum()\n",
    "    fe_ipca_lags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
