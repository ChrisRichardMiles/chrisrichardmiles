{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a4d2dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "test = pd.read_csv('input/test.csv')\n",
    "sample_submission = pd.read_csv('input/sample_submission.csv')\n",
    "book_test = pd.read_parquet('input/book_test.parquet/stock_id=0')\n",
    "trade_test = pd.read_parquet('input/trade_test.parquet/stock_id=0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477c3c3e",
   "metadata": {},
   "source": [
    "# Optiver Realized Volatility Prediction: 91st place solution\n",
    "\n",
    "* Competition website: [https://www.kaggle.com/competitions/optiver-realized-volatility-prediction/overview](https://www.kaggle.com/competitions/optiver-realized-volatility-prediction/overview)\n",
    "* Evaluation details: [https://www.kaggle.com/competitions/optiver-realized-volatility-prediction/overview/evaluation](https://www.kaggle.com/competitions/optiver-realized-volatility-prediction/overview/evaluation)\n",
    "* Data details: [https://www.kaggle.com/competitions/optiver-realized-volatility-prediction/overview/evaluation](https://www.kaggle.com/competitions/optiver-realized-volatility-prediction/data)\n",
    "* My submission: [https://www.kaggle.com/code/chrisrichardmiles/opt-inf-ensemble-final-1](https://www.kaggle.com/code/chrisrichardmiles/opt-inf-ensemble-final-1)\n",
    "\n",
    "## What does the host of this competition want?\n",
    "Predictions of the volatility of stock prices over the next 10 minute window, given trading data and book data.\n",
    "## Why volatility? \n",
    "Volatility is important because it is used in calculating the value of a stock option. We can trade more profitably if we are better at determining value. \n",
    "\n",
    "$$ \\textrm{option_value} = \\textrm{intrinsic_value} + \\textrm{time_value} $$\n",
    "\n",
    "Intrinsic value is just the difference between the current price of the stock and strike price of the option, so it is known at the time of sale. But the time value is harder to calculate. To calculate the time value, one would need to know the probability density distribution of stock price at the expiration time of the option. The volatility of a stock's price will affect that distribution since a stock with high volatility will have larger price changes over time. Therefore if we can better predict the volatility of a stock, we can trade options more profitably. \n",
    "\n",
    "## Precisely, what are we being asked to predict?\n",
    "### realized volatility\n",
    "We will compute the log returns over all consecutive book updates and we define the **realized volatility, $\\sigma$,** as the squared root of the sum of squared log returns.\n",
    "$$\n",
    "\\sigma = \\sqrt{\\sum_{t}r_{t-1, t}^2}\n",
    "$$\n",
    "### Log returns \n",
    "Calling $S_t$ the price of the stock $S$ at time $t$, we can define the log return between $t_1$ and $t_2$ as:\n",
    "$$\n",
    "r_{t_1, t_2} = \\log \\left( \\frac{S_{t_2}}{S_{t_1}} \\right)\n",
    "$$\n",
    "Where we use **WAP** (Weighted Average Price) as price of the stock to compute log returns.\n",
    "$$ WAP = \\frac{BidPrice_{1}*AskSize_{1} + AskPrice_{1}*BidSize_{1}}{BidSize_{1} + AskSize_{1}} $$\n",
    "where an order book looks like this: \n",
    "![order_book_1](https://www.optiver.com/wp-content/uploads/2021/05/OrderBook3.png)\n",
    "\n",
    "## How will our predictions be scored? \n",
    "Submissions are evaluated using the root mean square percentage error, defined as:\n",
    "\n",
    "$$\\text{RMSPE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} ((y_i - \\hat{y}_i)/y_i)^2}$$\n",
    "\n",
    "There will be around 100 stock ids in the test set and around 150,000 rows to predict.\n",
    "With `row_id` reffering to \"stock_id\"-\"time_id\", the submission file looks like: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bef64435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0-4</td>\n",
       "      <td>0.003048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0-32</td>\n",
       "      <td>0.003048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0-34</td>\n",
       "      <td>0.003048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  row_id    target\n",
       "0    0-4  0.003048\n",
       "1   0-32  0.003048\n",
       "2   0-34  0.003048"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d86f3e",
   "metadata": {},
   "source": [
    "## What does the input data look like at the time of prediction? \n",
    " \n",
    "\n",
    "### book_test.parque\n",
    "A parquet file partitioned by stock_id. Provides order book data on the most competitive buy and sell orders entered into the market. The top two levels of the book are shared. The first level of the book will be more competitive in price terms, it will then receive execution priority over the second level.\n",
    "\n",
    "Here are the first few rows of the book data for stock_id 0:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415e38ba",
   "metadata": {},
   "source": [
    "<!-- stock_id - ID code for the stock. Not all stock IDs exist in every time bucket. Parquet coerces this column to the categorical data type when loaded; you may wish to convert it to int8.\n",
    "time_id - ID code for the time bucket. Time IDs are not necessarily sequential but are consistent across all stocks.\n",
    "seconds_in_bucket - Number of seconds from the start of the bucket, always starting from 0.\n",
    "bid_price[1/2] - Normalized prices of the most/second most competitive buy level.\n",
    "ask_price[1/2] - Normalized prices of the most/second most competitive sell level.\n",
    "bid_size[1/2] - The number of shares on the most/second most competitive buy level.\n",
    "ask_size[1/2] - The number of shares on the most/second most competitive sell level. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fad7d4ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_id</th>\n",
       "      <th>seconds_in_bucket</th>\n",
       "      <th>bid_price1</th>\n",
       "      <th>ask_price1</th>\n",
       "      <th>bid_price2</th>\n",
       "      <th>ask_price2</th>\n",
       "      <th>bid_size1</th>\n",
       "      <th>ask_size1</th>\n",
       "      <th>bid_size2</th>\n",
       "      <th>ask_size2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000049</td>\n",
       "      <td>1.000590</td>\n",
       "      <td>0.999656</td>\n",
       "      <td>1.000639</td>\n",
       "      <td>91</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000049</td>\n",
       "      <td>1.000590</td>\n",
       "      <td>0.999656</td>\n",
       "      <td>1.000639</td>\n",
       "      <td>91</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000049</td>\n",
       "      <td>1.000639</td>\n",
       "      <td>0.999656</td>\n",
       "      <td>1.000885</td>\n",
       "      <td>290</td>\n",
       "      <td>20</td>\n",
       "      <td>101</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   time_id  seconds_in_bucket  bid_price1  ask_price1  bid_price2  ask_price2  \\\n",
       "0        4                  0    1.000049    1.000590    0.999656    1.000639   \n",
       "1        4                  1    1.000049    1.000590    0.999656    1.000639   \n",
       "2        4                  5    1.000049    1.000639    0.999656    1.000885   \n",
       "\n",
       "   bid_size1  ask_size1  bid_size2  ask_size2  \n",
       "0         91        100        100         24  \n",
       "1         91        100        100         20  \n",
       "2        290         20        101         15  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a4f80c",
   "metadata": {},
   "source": [
    "### trade_test.parquet \n",
    "A parquet file partitioned by stock_id. Contains data on trades that actually executed. Usually, in the market, there are more passive buy/sell intention updates (book updates) than actual trades, therefore one may expect this file to be more sparse than the order book.\n",
    "\n",
    "Here are the first few rows of the trade data for stock_id 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "990da2b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_id</th>\n",
       "      <th>seconds_in_bucket</th>\n",
       "      <th>price</th>\n",
       "      <th>size</th>\n",
       "      <th>order_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1.000344</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>1.000049</td>\n",
       "      <td>100</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>1.000059</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   time_id  seconds_in_bucket     price  size  order_count\n",
       "0        4                  7  1.000344     1            1\n",
       "1        4                 24  1.000049   100            7\n",
       "2        4                 27  1.000059   100            3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trade_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0773ef04",
   "metadata": {},
   "source": [
    "### test.csv \n",
    "Provides the mapping between the other data files and the submission file. As with other test files, most of the data is only available to your notebook upon submission with just the first few rows available for download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3535cdbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th>row_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>0-32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>0-34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stock_id  time_id row_id\n",
       "0         0        4    0-4\n",
       "1         0       32   0-32\n",
       "2         0       34   0-34"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2606d21d",
   "metadata": {},
   "source": [
    "#### Important notes about the data:\n",
    "\n",
    " * As stated on the [data page](https://www.kaggle.com/competitions/optiver-realized-volatility-prediction/data), Time IDs are not necessarily sequential but are consistent across all stocks. This is very important information. It means we can use information from all stocks in a given time_id, but can't create lagging features since we can't order the data by time. However, this turned out not to be completely true. The winner of the competiton used the price tick size to reverse engineer the order of the time_ids, as stated in their write up: [https://www.kaggle.com/competitions/optiver-realized-volatility-prediction/discussion/274970](https://www.kaggle.com/competitions/optiver-realized-volatility-prediction/discussion/274970). I did not use this technique in my solution.\n",
    " \n",
    "\n",
    " * Data for all time_ids are given in one batch, instead of sequentially. This is not how predictions are made in the real world, where you need to make predictions and act on them before you have any data from the future. Even though the time_ids are shuffled, we can still use aggregated information accross all time_ids when making predictions. I did take advantage of this in my solution by aggregating data across all time_ids."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47c2d42",
   "metadata": {},
   "source": [
    "## My solution \n",
    "### How to reproduce results\n",
    "To predict on the final test data, you must make a submission through kaggle.\n",
    "\n",
    "The script that produces the final predictions for competition test set: [https://www.kaggle.com/code/chrisrichardmiles/opt-inf-ensemble-final-1/notebook?scriptVersionId=75800583](https://www.kaggle.com/code/chrisrichardmiles/opt-inf-ensemble-final-1/notebook?scriptVersionId=75800583)\n",
    "\n",
    "\n",
    "The last link is to the last step in the pipeline. All of the model training scripts are public. They can be found in the input section of that notebook, also linked here: [https://www.kaggle.com/code/chrisrichardmiles/opt-inf-ensemble-final-1/data?scriptVersionId=75800583](https://www.kaggle.com/code/chrisrichardmiles/opt-inf-ensemble-final-1/data?scriptVersionId=75800583)\n",
    "\n",
    "The rest of the pipeline is also public:\n",
    "* Feature generation: [https://www.kaggle.com/chrisrichardmiles/generate-train-features-script-p13](https://www.kaggle.com/chrisrichardmiles/generate-train-features-script-p13)\n",
    "* Module opt_fe.py: [https://www.kaggle.com/code/chrisrichardmiles/opt-fe](https://www.kaggle.com/code/chrisrichardmiles/opt-fe)\n",
    "* Module opt_utils.py: [https://www.kaggle.com/code/chrisrichardmiles/opt-utils](https://www.kaggle.com/code/chrisrichardmiles/opt-utils)\n",
    "\n",
    "### Simple model reproduction\n",
    "* Requirements: 16 GB of RAM and minconda installed\n",
    "\n",
    "I have provided a script that will process the raw input data into training features and train one of the best single models which scores 0.21986 rmspe on the test set and would place 177th in the competition. \n",
    "\n",
    "#### 177th place simple model  kaggle submission \n",
    "- Training: [https://www.kaggle.com/code/chrisrichardmiles/opt-train-dart-op-175-fold-0/notebook](https://www.kaggle.com/code/chrisrichardmiles/opt-train-dart-op-175-fold-0/notebook)\n",
    "- Submission: [https://www.kaggle.com/code/chrisrichardmiles/opt-best-dart-inference/notebook](https://www.kaggle.com/code/chrisrichardmiles/opt-best-dart-inference/notebook)\n",
    "\n",
    "#### Instructions to build model locally\n",
    "* Clone this repository and navigate to the optiver directory: \n",
    "```\n",
    "git clone https://github.com/ChrisRichardMiles/chrisrichardmiles.git\n",
    "cd chrisrichardmiles/projects/optiver\n",
    "```\n",
    "\n",
    "* The competition data can be found here: [https://www.kaggle.com/competitions/optiver-realized-volatility-prediction/overview/evaluation](https://www.kaggle.com/competitions/optiver-realized-volatility-prediction/data)\n",
    "* Download data into the input folder. If you have the kaggle api installed, with your kaggle.json file in /root/.kaggle, you can run the following:\n",
    "```\n",
    "kaggle competitions download -p input -c optiver-realized-volatility-prediction\n",
    "```\n",
    "* Remove duplicate files and unzip the data\n",
    "```\n",
    "rm -rf input/*.parquet input/*.csv\n",
    "unzip -d input -q input/optiver-realized-volatility-prediction.zip\n",
    "```\n",
    "* If you'd like to save the metadata and results of the model training at neptune.ai, run the following with correct edits for your neptune project name and api token: \n",
    "```\n",
    "export NEPTUNE_PROJECT='neputune_username/project_name'\n",
    "export NEPTUNE_API_TOKEN='YOUR_LONG_API_TOKEN'\n",
    "```\n",
    "* Create the conda environment\n",
    "```\n",
    "conda env create -f env.yml\n",
    "conda activate opt\n",
    "```\n",
    "* Run `run.sh` to create features and train the model\n",
    "```\n",
    "bash run.sh\n",
    "```\n",
    "This will create train features as a pickle file called p13_train.pkl and then train the lightgbm-dart model, with the output stored in the folder `dart_model`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b5d290",
   "metadata": {},
   "source": [
    "I used gradient boosted tree based models and neural networks. For gradient boosting, I used the [Lightgbm](https://lightgbm.readthedocs.io/en/v3.3.2/) implementation, using both standard gradient boosting and dart mode. For the neural networks, I used the tabnet architecture, implemented by [Optimo](https://github.com/Optimox), at this repository: [https://github.com/dreamquark-ai/tabnet](https://github.com/dreamquark-ai/tabnet).\n",
    "\n",
    "* Dart paper: [https://arxiv.org/pdf/1505.01866.pdf](https://arxiv.org/pdf/1505.01866.pdf) K. V. Rashmi, Ran Gilad-Bachrach (2015)\n",
    "* Tabnet paper: [https://arxiv.org/pdf/1908.07442.pdf](https://arxiv.org/pdf/1908.07442.pdf) Arik, S. O., & Pfister, T. (2019)\n",
    "\n",
    "My final submission is a blend of predictions from 10 different models. The blend is 1/2(weighted arithmetic mean) + 1/2(weighted geometric mean). The weights are determined using the [minimize](https://docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.optimize.minimize.html) function from scipy. All models have the same input features, generated by the `p13` function in fe.py. \n",
    "\n",
    "### Overview of features\n",
    "\n",
    "### Models\n",
    "\n",
    "#### Regular models trained with 5-fold cross validation.\n",
    "* dart: lightgbm model with dart mode, using `lgb_params`\n",
    "* dart_175: lightgbm model with dart mode, using `dart_175_params`\n",
    "* tab: tabnet model with `tabnet_params`\n",
    "* tab_181: same as tab, except `T_0` is changed from 200 to 30 in `scheduler_params`\n",
    "* tab_183: same as tab_181 except `n_steps` is changed from 2 to 3.\n",
    "\n",
    "#### Nested models trained with nested cross validation\n",
    "* tab_nested: same as tab, but nested cv.\n",
    "* dart_nested: same as dart, but nested cv.\n",
    "* dart_175_nested: same as dart_175, but nested cv.\n",
    "\n",
    "#### Level 2 models:\n",
    "These models have the same features as the previous models, except with added predictions from tab_nested and dart_nested\n",
    "* dart_l2: same params as dart\n",
    "* lgb_l2: same as dart_l2, except `boosting_type`=`gbdt`\n",
    "\n",
    "#### Parameters \n",
    "```\n",
    "lgb_params = {\n",
    "        \"boosting_type\": \"dart\",\n",
    "        \"objective\": \"rmse\",\n",
    "        \"learning_rate\": .05,\n",
    "        \"num_leaves\": 255,\n",
    "        \"min_data_in_leaf\": 255,\n",
    "        \"feature_fraction\": 0.8,\n",
    "        \"bagging_fraction\": .5,\n",
    "        \"bagging_freq\": 1,      \n",
    "        \"n_estimators\": 10_000,\n",
    "        \"early_stopping_rounds\": 400,\n",
    "        \"n_jobs\": -1,\n",
    "        \"seed\": 42,\n",
    "        \"verbose\": -1, \n",
    "    }\n",
    "    \n",
    "dart_175 = {\n",
    "        \"boosting_type\": \"dart\",\n",
    "        \"objective\": \"rmse\",\n",
    "        \"learning_rate\": .05,\n",
    "        \"num_leaves\": 255,\n",
    "        \"min_data_in_leaf\": 2 ** 10,\n",
    "        \"feature_fraction\": 0.25,\n",
    "        \"bagging_fraction\": .85, \n",
    "        \"bagging_freq\": 1,      \n",
    "        \"n_estimators\": 10_000,\n",
    "        \"early_stopping_rounds\": 400,\n",
    "        \"n_jobs\": -1,\n",
    "        \"seed\": 42,\n",
    "        \"verbose\": -1, \n",
    "    }\n",
    "    \n",
    "tabnet_params = {\n",
    "        'cat_emb_dim': 1,\n",
    "        'n_d': ND_NA,\n",
    "        'n_a': ND_NA,\n",
    "        'n_steps': 2,\n",
    "        'gamma': 2,\n",
    "        'n_independent': 2,\n",
    "        'n_shared': 2,\n",
    "        'lambda_sparse': 0,\n",
    "        'optimizer_fn': Adam,\n",
    "        'optimizer_params': {'lr': 0.02},\n",
    "        'mask_type': 'entmax',\n",
    "        'scheduler_params': {\n",
    "            'T_0': 200,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0.0001,\n",
    "            'last_epoch': -1,\n",
    "            'verbose': False\n",
    "        }\n",
    "```\n",
    "\n",
    "#### K-fold cross validation brief explanation:\n",
    "We divide the data into k equally sized \"folds\". For each fold, we use the data in that fold as a validation set and use the remaining data to train a model. So for each set of hyperparameters (model, features, settings etc...) we train k models. When predicting a test set or production set of data, we take an average of all k model predictions. \n",
    "\n",
    "* **Note on early stopping**: When we are training models, we can choose to use the validation set to choose the optimal number of training iterations (number of tree boosters for gradient boosting and number of epochs for neural nets). This comes at the risk of overconfident cross validation scores. we can also retrain a single model with all the data for training, using the average number of iterations found with early stopping. After a few experiments, judging the outcome by nested cross validation score and the public leaderboard score, I found that using early stopping worked best in this competition.\n",
    "\n",
    "#### Nested cross validation brief explanation:\n",
    "This method of cross validation allows us to get out of fold predictions on a validation set, but use a separate validation set for early stopping. The benefit is that our cross validation predictions are not \"overfit\" due to early stopping on the same data, giving more robust validation as well as providing features that can be used for model stacking. The drawback of this method is that you must create k * (k - 1) models, instead of just k models needed for regular k-fold cross validation.\n",
    "* **example with 5 folds**:\n",
    "To obtain predictions for fold_5, I train 4 models and take a mean of their predictions. Each model sees 3 of the 4 remaining folds as training data with the remaining fold is a validation set for early stopping. The training/validaton splits look like [123|4], [124|3], [134|2], [234|1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee485bdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
