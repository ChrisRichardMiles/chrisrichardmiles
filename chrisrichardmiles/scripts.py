# AUTOGENERATED! DO NOT EDIT! File to edit: projects/m5/scripts.ipynb (unless otherwise specified).

__all__ = ['test_add', 'test', 'make_data_dir', 'hello_world']

# Cell
#export
import os
from fastcore.script import call_parse, Param
import neptune.new as neptune

# Cell
@call_parse
def test_add(a:Param("param a", int), b:Param("param 1",int)): return a + b

# Cell
@call_parse
def test(x:Param('a number', int)=3, y:Param('num', int)=4):
    x = int(x)
    print(x + y)
    return x + y

# Cell
@call_parse
def make_data_dir():
    os.makedirs(f'{name}/raw', exist_ok=True)
    os.makedirs(f'{name}/interim', exist_ok=True)
    os.makedirs(f'{name}/features', exist_ok=True)

# Cell
@call_parse
def hello_world():
    run = neptune.init(project='chrisrichardmiles/crm',
                 api_token='eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI5MzI4ZjliYy1kYWQ4LTQzNDMtYTQ2Ny02ZjBiODM0YjMzZjUifQ==') # your credentials

    # Track metadata and hyperparameters of your Run
    run["JIRA"] = "NPT-952"
    run["algorithm"] = "ConvNet"

    params = {
        "batch_size": 64,
        "dropout": 0.2,
        "learning_rate": 0.001,
        "optimizer": "Adam"
    }
    run["parameters"] = params


    # Track the training process by logging your training metrics
    for epoch in range(100):
        run["train/accuracy"].log(epoch * 0.6)
        run["train/loss"].log(epoch * 0.4)

    # Log the final results
    run["f1_score"] = 0.66

    # Stop logging to your Run
    run.stop()